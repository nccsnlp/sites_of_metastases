{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f59bfe6",
   "metadata": {},
   "source": [
    "# Simple Transformers for Site of Mets Model (Multi-Label Classification)\n",
    "This script is used to fine-tune BioBERT model for sites of mets multilabel classification using simpletransformers library (https://simpletransformers.ai/). You may replace BioBERT with other bert models supported in simpletransformers library for experimentation. Download required bert models from huggingface and place it in your local folder for offline training.\n",
    "\n",
    "Data source: radiology report\n",
    "\n",
    "Text column: conclusion section\n",
    "\n",
    "Label column: true_site_of_mets (list of metastatic sites, eg [\"site1\",\"site2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285517c",
   "metadata": {},
   "source": [
    "## Set-up environment\n",
    "First, we install the libraries which we'll use: !pip install simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859200ec",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f36fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import (MultiLabelClassificationArgs, MultiLabelClassificationModel)\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, classification_report, roc_auc_score, f1_score, precision_score,recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import datetime\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e4bc7",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer example.csv on the data format\n",
    "# true_site_of_mets contains a list of sites, eg [\"breast\",\"bone\"], use converters to read in as list, else it will be strings.\n",
    "# dataset contains either train / dev\n",
    "\n",
    "train_data = pd.read_csv(r'./data/train.csv', usecols=[\"report_id\",\"study_id\",\"conclusion\",\"true_site_of_mets\"], converters={\"true_site_of_mets\":eval})\n",
    "dev_data = pd.read_csv(r'./data/dev.csv', usecols=[\"report_id\",\"study_id\",\"conclusion\",\"true_site_of_mets\"], converters={\"true_site_of_mets\":eval})\n",
    "test_data = pd.read_csv(r'./data/test.csv', usecols=[\"report_id\",\"study_id\",\"conclusion\",\"true_site_of_mets\"], converters={\"true_site_of_mets\":eval})\n",
    "\n",
    "train_data.shape, dev_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"true_site_of_mets\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df  = train_data.copy()\n",
    "dev_df  = dev_data.copy()\n",
    "test_df  = test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f56e8",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744e395",
   "metadata": {},
   "source": [
    "### Multi-hot encoding for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cab6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(train_df['true_site_of_mets'])\n",
    "cols = [\"%s\" % c for c in mlb.classes_]\n",
    "num_labels = len(cols)\n",
    "print(num_labels)\n",
    "\n",
    "# Fit data into binarizer, generate multi-hot encodings\n",
    "df = pd.DataFrame(mlb.fit_transform(train_df['true_site_of_mets']), columns=mlb.classes_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge original text with multi-hot encodings\n",
    "train_df_wlabels = pd.concat([train_df[['conclusion']], df], axis=1)\n",
    "train_df_columns = train_df_wlabels.columns\n",
    "\n",
    "# Generate labels columns as list\n",
    "count = len(cols)\n",
    "train_df_wlabels['labels'] = ''\n",
    "\n",
    "for (i, row) in train_df_wlabels.iterrows():\n",
    "    labels = []\n",
    "    j = 1\n",
    "    while j <= count:\n",
    "        labels.append(train_df_wlabels.iloc[i].iloc[j])\n",
    "        j += 1\n",
    "    tup = tuple(labels)\n",
    "    train_df_wlabels.at[i, 'labels'] = tup\n",
    "\n",
    "# output individual label columns also\n",
    "#train_df_wlabels = train_df_wlabels[['conclusion', 'labels']]\n",
    "\n",
    "print(train_df_wlabels.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5555cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df_wlabels['labels'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc1113",
   "metadata": {},
   "source": [
    "### Multi-hot encoding for dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9794c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data into binarizer, generate multi-hot encodings\n",
    "df2 = pd.DataFrame(mlb.transform(dev_df['true_site_of_mets']),columns=mlb.classes_)\n",
    "print(df2.columns)\n",
    "\n",
    "# Merge original text with multi-hot encodings\n",
    "dev_df_wlabels = pd.concat([dev_df[['conclusion']], df2], axis=1)\n",
    "dev_df_columns = dev_df_wlabels.columns\n",
    "\n",
    "# Generate labels columns as list\n",
    "count = len(df2.columns)\n",
    "dev_df_wlabels['labels'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3377b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, row) in dev_df_wlabels.iterrows():\n",
    "    labels = []\n",
    "    j = 1\n",
    "    while j <= count:\n",
    "        labels.append(dev_df_wlabels.iloc[i].iloc[j])\n",
    "        j += 1\n",
    "    tup = tuple(labels)\n",
    "    dev_df_wlabels.at[i, 'labels'] = tup\n",
    "\n",
    "# output individual label columns also\n",
    "#dev_df_wlabels = dev_df_wlabels[['conclusion', 'labels']]\n",
    "\n",
    "print(dev_df_wlabels.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589b02c",
   "metadata": {},
   "source": [
    "### Multi-hot encoding for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58127460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data into binarizer, generate multi-hot encodings\n",
    "df3 = pd.DataFrame(mlb.transform(test_df['true_site_of_mets']),columns=mlb.classes_)\n",
    "print(df3.columns)\n",
    "\n",
    "# Merge original text with multi-hot encodings\n",
    "test_df_wlabels = pd.concat([test_df[['conclusion']], df3], axis=1)\n",
    "test_df_columns = test_df_wlabels.columns\n",
    "\n",
    "# Generate labels columns as list\n",
    "count = len(df3.columns)\n",
    "test_df_wlabels['labels'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, row) in test_df_wlabels.iterrows():\n",
    "    labels = []\n",
    "    j = 1\n",
    "    while j <= count:\n",
    "        labels.append(test_df_wlabels.iloc[i].iloc[j])\n",
    "        j += 1\n",
    "    tup = tuple(labels)\n",
    "    test_df_wlabels.at[i, 'labels'] = tup\n",
    "\n",
    "# output individual label columns also\n",
    "#test_df_wlabels = test_df_wlabels[['conclusion', 'labels']]\n",
    "\n",
    "print(test_df_wlabels.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12990390",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_wlabels.to_csv(\"./data/train_wlabels.csv\", index=False)\n",
    "dev_df_wlabels.to_csv(\"./data/dev_wlabels.csv\", index=False)\n",
    "test_df_wlabels.to_csv(\"./data/test_wlabels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78350576",
   "metadata": {},
   "source": [
    "## BioBert\n",
    "### do rename the filename_prefix to appropriate file directory for output file\n",
    "### change use_cuda=True for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6346a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select conclusion and labels columns for training\n",
    "train_df_wlabels = train_df_wlabels[['conclusion', 'labels']]\n",
    "dev_df_wlabels = dev_df_wlabels[['conclusion', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f4401",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_wlabels.shape, dev_df_wlabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import (\n",
    "    MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406dca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model configuration\n",
    "model_args = MultiLabelClassificationArgs()\n",
    "model_args.num_train_epochs = 10 #10\n",
    "model_args.learning_rate = 4e-5\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.evaluate_during_training = False\n",
    "model_args.manual_seed = 4\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.train_batch_size = 4\n",
    "model_args.eval_batch_size = 4\n",
    "model_args.max_seq_length = 512\n",
    "model_args.threshold = 0.5 \n",
    "\n",
    "model_args.n_gpu = 1\n",
    "\n",
    "#28-jun-2023 test1 earlystopping to prevent model overfitting\n",
    "model_args.use_early_stopping = True\n",
    "model_args.early_stopping_delta = 0.01\n",
    "model_args.early_stopping_metric = \"eval_loss\"\n",
    "model_args.early_stopping_metric_minimize = False\n",
    "model_args.early_stopping_patience = 5\n",
    "model_args.evaluate_during_training_steps = 500\n",
    "\n",
    "model_type = \"bert\"\n",
    "model_name=r\"path\\to\\yourlocalfolder\\biobert-base-cased-v1.2\"\n",
    "\n",
    "# Create a MultiLabelClassificationModel\n",
    "model = MultiLabelClassificationModel(\n",
    "    model_type,\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    args=model_args,\n",
    ")\n",
    "\n",
    "train_df_wlabels.columns = [\"conclusion\", \"labels\"]\n",
    "dev_df_wlabels.columns = [\"conclusion\", \"labels\"]\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df_wlabels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(dev_df_wlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = dev_df_wlabels['conclusion'].values.tolist()\n",
    "\n",
    "# Predict output\n",
    "prediction, outputs = model.predict(prediction_df)\n",
    "outputs_df = pd.DataFrame(outputs, columns=cols)\n",
    "prediction_df = pd.DataFrame(prediction, columns=cols)\n",
    "\n",
    "# Save outputs to csv file\n",
    "filename_prefix = \"./outputs/\" + \"biobert_outputs_df\"\n",
    "filename = \"%s.csv\" % filename_prefix\n",
    "outputs_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save true and predicted labels to csv file\n",
    "combined_cols_df = pd.concat([dev_df_wlabels, prediction_df], axis=1)\n",
    "filename_prefix = \"./outputs/\" + \"biobert_combined_cols_df\"\n",
    "filename = \"%s.csv\" % filename_prefix\n",
    "combined_cols_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5ea93",
   "metadata": {},
   "source": [
    "## get model metrics (DEV_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dev prediction csv\n",
    "combined_cols_df = pd.read_csv(\"./outputs/biobert_combined_cols_df.csv\",converters={\"labels\":eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cols_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e52533",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = combined_cols_df.columns[3:]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate individual label accuracies\n",
    "prediction_df = combined_cols_df.copy()\n",
    "\n",
    "# add predicted_labels tuple\n",
    "prediction_df[\"biobert_labels\"]=\"\"\n",
    "\n",
    "for (i, row) in prediction_df.iterrows():\n",
    "    labels = []\n",
    "    j = 3\n",
    "    while j <= len(cols)-1+3:\n",
    "        labels.append(prediction_df.iloc[i].iloc[j])\n",
    "        j += 1\n",
    "    tup = tuple(labels)\n",
    "    prediction_df.at[i, 'biobert_labels'] = tup\n",
    "\n",
    "prediction_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ef79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true is a string if read in the csv without converter, need to convert to tuple\n",
    "#y_true = prediction_df[\"labels\"].apply(lambda x: eval(x)).values.tolist()\n",
    "y_true = prediction_df[\"labels\"].values.tolist()\n",
    "\n",
    "# y_pred is already a tuple\n",
    "y_pred = prediction_df[\"biobert_labels\"].values.tolist()\n",
    "len(y_true), len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e178fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "label_ranking_average_precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get classification report\n",
    "print(classification_report(y_true,y_pred, target_names=cols, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322954e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_accuracy_score = accuracy_score(y_true,y_pred)\n",
    "micro_precision_score = precision_score(y_true,y_pred, average='micro')\n",
    "micro_recall_score = recall_score(y_true,y_pred, average='micro')\n",
    "micro_f1_score= f1_score(y_true,y_pred, average='micro')\n",
    "print(\"overall_accuracy_score: \", overall_accuracy_score)\n",
    "print(\"micro_precision_score: \", micro_precision_score)\n",
    "print(\"micro_recall_score: \", micro_recall_score)\n",
    "print(\"micro_f1_score: \",micro_f1_score)\n",
    "\n",
    "sample_precision_score = precision_score(y_true,y_pred, average='samples')\n",
    "sample_recall_score = recall_score(y_true,y_pred, average='samples')\n",
    "sample_f1_score= f1_score(y_true,y_pred, average='samples')\n",
    "print(\"sample_precision_score: \", sample_precision_score)\n",
    "print(\"sample_recall_score: \", sample_recall_score)\n",
    "print(\"sample_f1_score: \",sample_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[0][1], cols[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(cols)\n",
    "i = 0\n",
    "colnames = []\n",
    "accuracies = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1= []\n",
    "auroc = []\n",
    "\n",
    "while i < len(cols):\n",
    "    print(\"col: \", cols[i])\n",
    "    # extract col i\n",
    "    actualValue = [x[i] for x in y_true]\n",
    "    #print(actualValue)\n",
    "    predictedValue = [x[i] for x in y_pred]   \n",
    "    #print(predictedValue)\n",
    "    acc = accuracy_score(actualValue, predictedValue)\n",
    "    prec = precision_score(actualValue, predictedValue)\n",
    "    rc = recall_score(actualValue, predictedValue)\n",
    "    f = f1_score(actualValue, predictedValue)\n",
    "\n",
    "    print(\"***accuracy: \", acc)\n",
    "    # temporary fix, try-except block will be removed in the future with a more balanced dataset\n",
    "    try:\n",
    "        auroc_score = roc_auc_score(actualValue, predictedValue)\n",
    "        print(\"***auroc_score: \", auroc_score)\n",
    "    except ValueError:\n",
    "        auroc_score = 0\n",
    "    colnames.append(cols[i])\n",
    "    accuracies.append(acc)\n",
    "    precision.append(prec)\n",
    "    recall.append(rc)\n",
    "    f1.append(f)\n",
    "    auroc.append(auroc_score)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68451e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_auroc_df = pd.DataFrame(list(zip(colnames, accuracies, auroc,precision,recall,f1)), columns=['biobert_labels', 'accuracy','auc_roc_score','precision','recall','f1'])\n",
    "accuracy_auroc_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6131d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get biobert inverse labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(train_df['true_site_of_mets'])\n",
    "cols = [\"%s\" % c for c in mlb.classes_]\n",
    "print(len(cols))\n",
    "mlb.inverse_transform(np.asarray([prediction_df[\"biobert_labels\"].iloc[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a07cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[\"biobert_labels_inverse\"] = prediction_df[\"biobert_labels\"].apply(lambda x: mlb.inverse_transform(np.asarray([x])))\n",
    "prediction_df[\"labels_inverse\"] = prediction_df[\"labels\"].apply(lambda x: mlb.inverse_transform(np.asarray([x])))\n",
    "prediction_df.head(1)                                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv(\"./outputs/biobert_dev_prediction_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa2009",
   "metadata": {},
   "source": [
    "## Load best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d54c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./outputs/checkpoint-5880-epoch-10\"\n",
    "model = MultiLabelClassificationModel(\"bert\",output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d240d",
   "metadata": {},
   "source": [
    "## get model metrics (TEST_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5218ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = test_df_wlabels['conclusion'].values.tolist()\n",
    "\n",
    "# Predict output\n",
    "prediction, outputs = model.predict(prediction_df)\n",
    "outputs_df = pd.DataFrame(outputs, columns=cols)\n",
    "prediction_df = pd.DataFrame(prediction, columns=cols)\n",
    "\n",
    "# Save outputs to csv file\n",
    "filename_prefix = \"./outputs/\" + \"biobert_test_prediction_df\"\n",
    "filename = \"%s.csv\" % filename_prefix\n",
    "outputs_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60523f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add predicted_labels tuple\n",
    "prediction_df[\"biobert_labels\"]=\"\"\n",
    "\n",
    "for (i, row) in prediction_df.iterrows():\n",
    "    labels = []\n",
    "    j = 0\n",
    "    while j <= len(cols)-1:\n",
    "        labels.append(prediction_df.iloc[i].iloc[j])\n",
    "        j += 1\n",
    "    tup = tuple(labels)\n",
    "    prediction_df.at[i, 'biobert_labels'] = tup\n",
    "\n",
    "prediction_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5601b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48972ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df_wlabels[\"labels\"].values.tolist()\n",
    "y_pred = prediction_df[\"biobert_labels\"].values.tolist()\n",
    "len(y_true), len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998529a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "label_ranking_average_precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5411c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get classification report\n",
    "print(classification_report(y_true,y_pred, target_names=cols, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261fa40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_accuracy_score = accuracy_score(y_true,y_pred)\n",
    "micro_precision_score = precision_score(y_true,y_pred, average='micro')\n",
    "micro_recall_score = recall_score(y_true,y_pred, average='micro')\n",
    "micro_f1_score= f1_score(y_true,y_pred, average='micro')\n",
    "print(\"overall_accuracy_score: \", overall_accuracy_score)\n",
    "print(\"micro_precision_score: \", micro_precision_score)\n",
    "print(\"micro_recall_score: \", micro_recall_score)\n",
    "print(\"micro_f1_score: \",micro_f1_score)\n",
    "\n",
    "sample_precision_score = precision_score(y_true,y_pred, average='samples')\n",
    "sample_recall_score = recall_score(y_true,y_pred, average='samples')\n",
    "sample_f1_score= f1_score(y_true,y_pred, average='samples')\n",
    "print(\"sample_precision_score: \", sample_precision_score)\n",
    "print(\"sample_recall_score: \", sample_recall_score)\n",
    "print(\"sample_f1_score: \",sample_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e6228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "prediction_df[\"biobert_labels_inverse\"] = prediction_df[\"biobert_labels\"].apply(lambda x: mlb.inverse_transform(np.asarray([x])))\n",
    "# clean biobert_labels_inverse\n",
    "prediction_df[\"biobert_labels_inverse_clean\"] = prediction_df[\"biobert_labels_inverse\"].apply(lambda x: set(','.join([item for sublist in x for item in sublist]).replace(\"'\",\"\").split(\",\")))\n",
    "prediction_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c07b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7544ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save true and predicted labels to csv file\n",
    "combined_cols_df = pd.concat([test_df, prediction_df[\"biobert_labels_inverse_clean\"]], axis=1)\n",
    "filename_prefix = \"./outputs/\" + \"biobert_test_prediction_df_clean\"\n",
    "filename = \"%s.csv\" % filename_prefix\n",
    "combined_cols_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f228d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6be61a37",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54c704c0",
   "metadata": {},
   "source": [
    "for n in [1, 2, 3, 4, 5]:\n",
    "    for z in [0.001, 0.0001, 0.00001]:\n",
    "        for y in [4, 8, 16]:\n",
    "            for x in [64, 128, 256]:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d658c021",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for n in [1]:\n",
    "    for z in [0.0001]:\n",
    "        for y in [8]:\n",
    "            for x in [8]:\n",
    "    \n",
    "                # Capture start time\n",
    "                start = time.ctime()\n",
    "\n",
    "                curr_epoch = \"Epoch\" + str(n)\n",
    "\n",
    "                # Configure model args\n",
    "                model_args = MultiLabelClassificationArgs(num_train_epochs=n)\n",
    "                model_args.evaluate_during_training_steps = -1\n",
    "                model_args.save_eval_checkpoints = False\n",
    "                model_args.save_model_every_epoch = False\n",
    "                model_args.learning_rate = z\n",
    "                model_args.manual_seed = 4\n",
    "                model_args.multiprocessing_chunksize = 5000\n",
    "                model_args.no_cache = True\n",
    "                model_args.reprocess_input_data = True\n",
    "                model_args.train_batch_size = y\n",
    "                model_args.max_seq_length = x\n",
    "                model_args.gradient_accumulation_steps = 2\n",
    "                model_args.use_multiprocessing = True\n",
    "                model_args.overwrite_output_dir = True\n",
    "                model_args.labels_list = cols\n",
    "\n",
    "                # model\n",
    "                model_type = \"bert\"\n",
    "                model_name=\"C:\\Users\\guathwa\\ADAPET\\cache_huggingface\\biobert-base-cased-v1.2\"\n",
    "                #model_type = \"roberta\"\n",
    "                #model_name = \"roberta-large\"\n",
    "                #model_name = \"roberta-base\"\n",
    "                #model_type = \"bert\"\n",
    "                #model_name = \"bert-large-uncased\"\n",
    "                #model_type = \"xlnet\"\n",
    "                #model_name = \"xlnet-large-cased\"\n",
    "                \n",
    "                # Use for data naming only\n",
    "                arguments = \"_multiprocessingT_\" + \"_seqlen\" +str(model_args.max_seq_length)\\\n",
    "                            +\"_lr\"+str(model_args.learning_rate) +\"_batch\"+str(model_args.train_batch_size)\n",
    "                dataset = 'train4522'\n",
    "                \n",
    "                # Create Transformer Model\n",
    "                model = MultiLabelClassificationModel(model_type, model_name, num_labels=count, use_cuda=True, args=model_args)\n",
    "\n",
    "                if __name__ == '__main__':\n",
    "\n",
    "                    # Train the model\n",
    "                    model.train_model(train_df)\n",
    "\n",
    "                    prediction_df = dev_df['conclusion'].values.tolist()\n",
    "\n",
    "                    # Predict output\n",
    "                    prediction, outputs = model.predict(prediction_df)\n",
    "                    outputs_df = pd.DataFrame(outputs, columns=cols)\n",
    "                    prediction_df = pd.DataFrame(prediction, columns=cols)\n",
    "\n",
    "                    # Save outputs to csv file\n",
    "                    filename_prefix = \"./outputs/\" + model_name + \"2_outputs_df\"\n",
    "                    filename = \"%s.csv\" % filename_prefix\n",
    "                    outputs_df.to_csv(filename)\n",
    "\n",
    "                    # Save true and predicted labels to csv file\n",
    "                    combined_cols_df = pd.concat([eval_df, prediction_df], axis=1)\n",
    "                    filename_prefix = \"./outputs/\" + model_name + \"2_combined_cols_df\"\n",
    "                    filename = \"%s.csv\" % filename_prefix\n",
    "                    combined_cols_df.to_csv(filename)\n",
    "\n",
    "                    # Calculate individual label accuracies\n",
    "                    label_accuracy_df = pd.concat([eval_df_individual_labels, prediction_df], axis=1)\n",
    "                    new_acc_cols_order = np.unique(np.array(list(zip(eval_df_individual_labels.columns, prediction_df.columns))).flatten())\n",
    "                    label_accuracy_df = label_accuracy_df[new_acc_cols_order]\n",
    "\n",
    "                    count = len(label_accuracy_df.columns)\n",
    "                    i = 0\n",
    "                    colnames = []\n",
    "                    accuracies = []\n",
    "                    auroc = []\n",
    "\n",
    "                    while i < count:\n",
    "                        actualValue = label_accuracy_df.iloc[:, i]\n",
    "                        predictedValue = label_accuracy_df.iloc[:, i + 1]\n",
    "                        actualValue = actualValue.values\n",
    "                        predictedValue = predictedValue.values\n",
    "                        acc = accuracy_score(actualValue, predictedValue)\n",
    "                        # temporary fix, try-except block will be removed in the future with a more balanced dataset\n",
    "                        try:\n",
    "                            auroc_score = roc_auc_score(actualValue, predictedValue)\n",
    "                        except ValueError:\n",
    "                            auroc_score = 0\n",
    "                        colnames.append(label_accuracy_df.columns[i])\n",
    "                        accuracies.append(acc)\n",
    "                        auroc.append(auroc_score)\n",
    "                        i += 2\n",
    "\n",
    "                    accuracy_auroc_df = pd.DataFrame(list(zip(colnames, accuracies, auroc)), columns=['Site', 'Accuracy','AUROC Score'])\n",
    "\n",
    "                    # Evaluate model\n",
    "                    result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "                    \n",
    "                    # Processing for metrics calculation\n",
    "                    eval_df_multi_hot_encodings = []\n",
    "\n",
    "                    for (i, row) in eval_df.iterrows():\n",
    "                        val = eval_df['labels'].iloc[i]\n",
    "                        val_array = np.asarray(val)\n",
    "                        eval_df_multi_hot_encodings.append(val_array)\n",
    "\n",
    "                    eval_df_true = np.array(eval_df_multi_hot_encodings)\n",
    "                    prediction_data = np.array(prediction)\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    overall_acc = accuracy_score(eval_df_true, prediction_data)\n",
    "                    # temporary fix, try-except block will be removed in the future with a more balanced dataset\n",
    "                    try:\n",
    "                        overall_auroc = roc_auc_score(eval_df_true, prediction_data)\n",
    "                    except:\n",
    "                        overall_auroc = 0\n",
    "                    hamming_loss = hamming_loss(eval_df_true, prediction_data)\n",
    "\n",
    "                    target_names = cols\n",
    "                    other_metrics_report = classification_report(eval_df_true, prediction_data, target_names=target_names, output_dict=True)\n",
    "                    classification_report_df = pd.DataFrame(other_metrics_report).transpose()\n",
    "\n",
    "                    # Combine individual accuracies to classification report\n",
    "                    classification_report_df = classification_report_df.reset_index()\n",
    "                    accuracy_auroc_df = accuracy_auroc_df.reset_index()        \n",
    "                    classification_report_df_final = pd.concat([classification_report_df, accuracy_auroc_df['Accuracy'],\n",
    "                                                                accuracy_auroc_df['AUROC Score']], axis=1)\n",
    "                    \n",
    "                    # Capture end time\n",
    "                    end = time.ctime()\n",
    "\n",
    "                    # Save classification report to csv file\n",
    "                    filename_prefix = \"./outputs/\" + model_name + \"2_classification_metrics_df\"\n",
    "                    filename = \"%s.csv\" % filename_prefix\n",
    "                    classification_report_df_final.to_csv(filename)\n",
    "\n",
    "                    # Save other metrics to csv file\n",
    "                    metrics_data = [model_type, model_name, curr_epoch, start, end, arguments, dataset, overall_acc, overall_auroc, hamming_loss, result['eval_loss']]\n",
    "                    df_length = len(eval_metrics_df)\n",
    "                    eval_metrics_df.loc[df_length] = metrics_data\n",
    "                    filename_prefix = \"./outputs/\" + model_name + \"2_eval_metrics_df\"\n",
    "                    filename = \"%s.csv\" % filename_prefix\n",
    "                    eval_metrics_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932de27b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
