{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf17f981",
   "metadata": {},
   "source": [
    "## Purpose: To train/evaluate a custom Radio NER model based on Annotated Radiology Reports Conclusion Text (using SparkNLP library v3.4.2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80a7e560",
   "metadata": {},
   "source": [
    "#sub folders\n",
    "    > dataset : contains input conll files\n",
    "    > graph : tensorflow graph file\n",
    "    > saved_models : ner models\n",
    "    > ner_output : model predictions\n",
    "    > ner_result : performance metrics\n",
    "    > ner_logs_ncc : training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to run to create the subfolders, for the first time\n",
    "#!mkdir graph saved_models ner_output ner_result ner_logs_ncc inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef2923",
   "metadata": {},
   "source": [
    "### Note: Before running this notebook, please configure the following paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using sparknlp clinical embedding word model\n",
    "# specify your folder containing the downloaded clinical embedding word model file, or you can use .pretrained during training instead to load it online\n",
    "embeddings_clinical_local_path = r\"path\\to\\sparknlp_pretrained\\embeddings_clinical_en_2.4.0_2.4_1580237286004\"\n",
    "\n",
    "# we are using sparknlp radiology assertion model\n",
    "# specify your folder containing the downloaded jsl assertion model file, or you can use .pretrained during training instead to load it online\n",
    "jsl_radiology_assertion = r\"path\\to\\\\sparknlp_pretrained\\assertion_dl_radiology_en_2.7.4_2.4_1616071311532\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your sparknlp online license key-need internet connection\n",
    "# we are using v3.4.2\n",
    "sparknlp_licence_key = r\"..\\sparknlp_licence_key\\yourkey.json\"\n",
    "\n",
    "# specify your sparknlp offline license key-airgap env\n",
    "# we are using v3.4.2\n",
    "sparknlp_airgap_licence_key = r\"..\\sparknlp_licence_key\\yourairgapkey.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test setting\n",
    "data_folder = \"dataset\"\n",
    "train_folder = data_folder+\"\\\\02conll\\conll_train\"\n",
    "test_file = data_folder+\"\\\\02conll\\conll_test\\\\test4522.txt\"\n",
    "dataset_name = \"train4522\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!dir $train_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!dir $test_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b105567",
   "metadata": {},
   "source": [
    "## Train/Evaluate NER Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5f363",
   "metadata": {},
   "source": [
    "Note: Requires Spark NLP and Spark NLP for Healthcare (licensed version) packages to be installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f987c2",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd737e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, sparknlp, sparknlp_jsl, ner_log_parser, datetime, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.training import CoNLL\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp_jsl.training import tf_graph\n",
    "from sparknlp_display import AssertionVisualizer, NerVisualizer \n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15305308",
   "metadata": {},
   "source": [
    "### Start Spark Session (Online with internet)\n",
    "- use this only if you need to download new models with .pretrained(..)\n",
    "- after that, put the new models' jar in local path, and load it using .load(..)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0be640b7",
   "metadata": {},
   "source": [
    "# Online-Load online license key\n",
    "with open(sparknlp_licence_key) as f:\n",
    "    license_keys = json.load(f)\n",
    "    \n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "os.environ.update(license_keys)\n",
    "\n",
    "# check variable\n",
    "!echo $SECRET\n",
    "!echo $JSL_VERSION\n",
    "!echo $PUBLIC_VERSION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03e8884d",
   "metadata": {},
   "source": [
    "# Start Spark Session with Custom Params (ONLINE)\n",
    "#         .config(\"spark.local.dir\", r\"c:\\users\\guathwa\\spark-temp\")\n",
    "\n",
    "def start():\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed\") \\\n",
    "        .master(\"local[16]\") \\\n",
    "        .config(\"spark.driver.memory\", \"30g\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\")\\\n",
    "        .config(\"spark.jars\", \"https://pypi.johnsnowlabs.com/\"+SECRET+\"/spark-nlp-jsl-\"+JSL_VERSION+\".jar\") \n",
    "        \n",
    "    return builder.getOrCreate()\n",
    "\n",
    "spark = start()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a7176",
   "metadata": {},
   "source": [
    "### Start Spark Session (OFFLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline-Load airgap license key\n",
    "with open(sparknlp_airgap_licence_key) as f:\n",
    "    airgap_license_keys = json.load(f)\n",
    "    \n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(airgap_license_keys)\n",
    "os.environ.update(airgap_license_keys)\n",
    "\n",
    "# check variable\n",
    "!echo $SECRET\n",
    "!echo $JSL_VERSION\n",
    "!echo $PUBLIC_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6ea39",
   "metadata": {},
   "source": [
    "Note: Requires Spark NLP for Healthcare (licensed version) license key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your downloaded spark nlp jar files in a local folder, eg d:\\content\n",
    "# note path cannot be too long, else there will be a java error on package not callable\n",
    "\n",
    "def start(SECRET):\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed radio ner\") \\\n",
    "        .master(\"local[16]\") \\\n",
    "        .config(\"spark.driver.memory\", \"60G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\") \\\n",
    "        .config(\"spark.jars\", f\"d:\\content\\spark-nlp-jsl-3.4.2.jar, d:\\content\\spark-nlp_2.12-3.4.2.jar\" ) ## change this\n",
    "\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = start(SECRET) \n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e824eafc",
   "metadata": {},
   "source": [
    "### Build a Tensorflow Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ed3bd",
   "metadata": {},
   "source": [
    "Note: Depending on the number of characters (nchars), a tensorflow graph may need to be build before the NER Model Training can take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec959ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to run for the first time\n",
    "#tf_graph.build(\"ner_dl\", build_params={\"embeddings_dim\": 200, \"nchars\":88, \"ntags\": 60, \"is_medical\": 1}, model_location=\"./graph/medical_ner_graphs\", model_filename=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58324d02",
   "metadata": {},
   "source": [
    "### Specify training sets to use\n",
    "- for the initial run, can use all sets to see the performance of each set, this will take about 15 hrs\n",
    "- after that, if need to rerun only certain training sets, then specify the params accordingly"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45e669d3",
   "metadata": {},
   "source": [
    "# this is for all training sets \n",
    "# use this to get model performance for all training sets to determine which is the best\n",
    "# change to code to run\n",
    "input_resample_params = ['u0.1o1','u0.1o2','u0.1o3',\n",
    "                        'u0.2o1','u0.2o2','u0.2o3',\n",
    "                        'u0.3o1','u0.3o2','u0.3o3',\n",
    "                        'u0.4o1','u0.4o2','u0.4o3',\n",
    "                        'u0.5o1','u0.5o2','u0.5o3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for selected training sets\n",
    "# use this to test run pipeline or to rerun specific training sets\n",
    "input_resample_params = ['u0.3o1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check training file path\n",
    "for resample_ratio in input_resample_params:\n",
    "    training_filepath = train_folder+\"\\\\\"+dataset_name + \"_\"+str(resample_ratio) + \".txt\"\n",
    "    print(training_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd775e",
   "metadata": {},
   "source": [
    "### check conll file before training\n",
    "- to confirm conll file is not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be0dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check conll file before training\n",
    "for resample_ratio in input_resample_params:\n",
    "    training_filepath = train_folder+\"\\\\\"+dataset_name + \"_\"+str(resample_ratio) + \".txt\"\n",
    "    \n",
    "    ###### Data Preparation\n",
    "    print('...filename: ',training_filepath )\n",
    "    training_data = CoNLL().readDataset(spark, training_filepath) \n",
    "\n",
    "    print(\"...Conll Dataset Count: \" + str(training_data.count()))\n",
    "\n",
    "    # Groupby view of conll_data \n",
    "    training_data.select(F.explode(F.arrays_zip('token.result','label.result')).alias(\"cols\")) \\\n",
    "    .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "            F.expr(\"cols['1']\").alias(\"ground_truth\")).groupBy('ground_truth').count().orderBy('count', ascending=False)\\\n",
    "            .show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test file name\n",
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec722470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check reading of test.txt\n",
    "# if there is Py4JavaError, open notebpad, copy all content to a new text file and save it as testxxxv2.txt\n",
    "test_file = data_folder+\"\\\\02conll\\conll_test\\\\test4522v2.txt\"\n",
    "test_data = CoNLL().readDataset(spark, test_file)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3fcf4",
   "metadata": {},
   "source": [
    "## ------------------- START OF TRAINING  --------------------"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a5faa21",
   "metadata": {},
   "source": [
    "# output files generated after training\n",
    "    > saved_models : ner models\n",
    "    > ner_output : model predictions (ground truth and predicted value)\n",
    "    > ner_result : performance metrics (accuracy, classification report)\n",
    "    > ner_logs_ncc : training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbca49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Output Dataframe to store the performance metrics\n",
    "output_df = pd.DataFrame(columns = ['model', 'epoch', 'learning_rate', 'batch_size', 'resampling_ratio', 'start_time', 'end_time', 'duration', 'overall_accuracy','class_accuracy', 'classification_report'])\n",
    "\n",
    "# test conll data\n",
    "test_data = CoNLL().readDataset(spark, test_file)\n",
    "\n",
    "for resample_ratio in input_resample_params:\n",
    "    train_file = train_folder+\"\\\\\"+dataset_name + \"_\"+str(resample_ratio) + \".txt\"\n",
    "    \n",
    "    ###### Data Preparation\n",
    "    training_data = CoNLL().readDataset(spark, train_file) \n",
    "\n",
    "    print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "    print(\"Test Dataset Count: \" + str(test_data.count()))\n",
    "\n",
    "    # Groupby view of training data\n",
    "    training_data.select(F.explode(F.arrays_zip('token.result','label.result')).alias(\"cols\")) \\\n",
    "    .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "            F.expr(\"cols['1']\").alias(\"ground_truth\")).groupBy('ground_truth').count().orderBy('count', ascending=False)\\\n",
    "            .show(100,truncate=False)\n",
    "\n",
    "    # Groupby view of testing data\n",
    "    test_data.select(F.explode(F.arrays_zip('token.result','label.result')).alias(\"cols\")) \\\n",
    "    .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "            F.expr(\"cols['1']\").alias(\"ground_truth\")).groupBy('ground_truth').count().orderBy('count', ascending=False)\\\n",
    "            .show(100,truncate=False)\n",
    "    \n",
    "    ## training loop\n",
    "    ## for test run of pipeline, use n=1, takes about 5 mins for u0.3o1 train file\n",
    "    ## for training, use n=5\n",
    "    for n in [1]:\n",
    "        for x in [8]:\n",
    "            for z in [0.001]:\n",
    "                epoch = n\n",
    "                batch_size = x\n",
    "                learning_rate = z\n",
    "                resample_ratio = resample_ratio\n",
    "                model_type = \"clinical_embeddings\"\n",
    "                start = time.ctime()\n",
    "                start2 = time.time()\n",
    "                print('Start of loop: ', 'epoch =', n, ', batch_size =', x, ', learning_rate =', z, ', resample_ratio =', resample_ratio)\n",
    "                print('Start time for new loop: ', start)\n",
    "                print(50*'-')\n",
    "                clinical_embeddings = WordEmbeddingsModel.load(embeddings_clinical_local_path)\\\n",
    "                          .setInputCols([\"sentence\", \"token\"])\\\n",
    "                          .setOutputCol(\"embeddings\")\n",
    "\n",
    "                filename_prefix = \"ner_dl_ncctest_\"  + model_type + str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
    "                clinical_embeddings.transform(test_data).write.parquet(\"%s.parquet\" % filename_prefix)\n",
    "\n",
    "                nerTagger = MedicalNerApproach()\\\n",
    "                            .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "                            .setLabelColumn(\"label\")\\\n",
    "                            .setOutputCol(\"ner\")\\\n",
    "                            .setMaxEpochs(n)\\\n",
    "                            .setLr(z)\\\n",
    "                            .setBatchSize(x)\\\n",
    "                            .setRandomSeed(100)\\\n",
    "                            .setVerbose(1)\\\n",
    "                            .setValidationSplit(0.1)\\\n",
    "                            .setEvaluationLogExtended(True) \\\n",
    "                            .setEnableOutputLogs(True)\\\n",
    "                            .setIncludeConfidence(True)\\\n",
    "                            .setTestDataset(\"%s.parquet\" % filename_prefix)\\\n",
    "                            .setOutputLogsPath('./ner_logs_ncc')\\\n",
    "                            .setGraphFolder('./graph/medical_ner_graphs')\\\n",
    "                            .setEarlyStoppingCriterion(0.01)\\\n",
    "                            .setEarlyStoppingPatience(3)                \n",
    "                ner_pipeline = Pipeline(stages=[\n",
    "                                clinical_embeddings,\n",
    "                                nerTagger])\n",
    "                ner_model_clinicalembeddings = ner_pipeline.fit(training_data)\n",
    "                test_data = clinical_embeddings.transform(test_data)\n",
    "                predictions = ner_model_clinicalembeddings.transform(test_data)\n",
    "\n",
    "                #===================================================\n",
    "                ## Save NER Model to be used in Pipeline ##done\n",
    "                filename_save = \"./saved_models/\"+ model_type + \"_\"  + str(epoch) + \"_\" + str(batch_size)\\\n",
    "                                   + \"_\" + str(learning_rate) + \"_\" + str(resample_ratio) + \"_\" + dataset_name\n",
    "\n",
    "                ner_model_clinicalembeddings.stages[1].write().overwrite().save(filename_save)               \n",
    "                #====================================================\n",
    "                \n",
    "                ## Output model predictions to csv file\n",
    "                filename_prefix =  \"./ner_output/\"+ model_type + \"_\"  + str(epoch) + \"_\" + str(batch_size)\\\n",
    "                                   + \"_\" + str(learning_rate) + \"_\" + str(resample_ratio) + \"_\" + dataset_name\n",
    "                filename = \"%s.csv\" % filename_prefix\n",
    "\n",
    "                predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\"))\\\n",
    "                       .select(F.expr(\"cols['0']\").alias(\"token\"), F.expr(\"cols['1']\").alias(\"ground_truth\"), F.expr(\"cols['2']\")\\\n",
    "                       .alias(\"prediction\")).toPandas().to_csv(filename)\n",
    "\n",
    "                preds_df =  predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\"))\\\n",
    "                       .select(F.expr(\"cols['0']\").alias(\"token\"), F.expr(\"cols['1']\").alias(\"ground_truth\"), F.expr(\"cols['2']\")\\\n",
    "                       .alias(\"prediction\")).toPandas()\n",
    "\n",
    "                report = classification_report(preds_df['ground_truth'], preds_df['prediction'], digits=4)\n",
    "                accuracy = accuracy_score(preds_df['ground_truth'] , preds_df['prediction'])\n",
    "                \n",
    "                # get per class accuracy\n",
    "                # https://stackoverflow.com/questions/39770376/scikit-learn-get-accuracy-scores-for-each-class\n",
    "                classes = np.unique(preds_df['ground_truth'])\n",
    "                cm = confusion_matrix(preds_df['ground_truth'], preds_df['prediction']) \n",
    "\n",
    "                # We will store the results in a dictionary for easy access later\n",
    "                per_class_accuracies = {}\n",
    "\n",
    "                # Calculate the accuracy for each one of our classes\n",
    "                for idx, cls in enumerate(classes):\n",
    "                    # True negatives are all the samples that are not our current GT class (not the current row) \n",
    "                    # and were not predicted as the current class (not the current column)\n",
    "                    true_negatives = np.sum(np.delete(np.delete(cm, idx, axis=0), idx, axis=1))\n",
    "\n",
    "                    # True positives are all the samples of our current GT class that were predicted as such\n",
    "                    true_positives = cm[idx, idx]\n",
    "\n",
    "                    # The accuracy for the current class is ratio between correct predictions to all predictions                   \n",
    "                    # 03-jul-2023: dont consider TN, use TP/(TP+FP+FN), same formulae for whole manuscript\n",
    "                    per_class_accuracies[cls] = (true_positives) / (np.sum(cm)-true_negatives)                     \n",
    "\n",
    "                # Combine class accuracies to classification report\n",
    "                report_dict = classification_report(preds_df['ground_truth'], preds_df['prediction'], digits=4, labels=classes, output_dict=True)\n",
    "                classification_report_df = pd.DataFrame(report_dict).transpose()\n",
    "                per_class_accuracies_df = pd.DataFrame.from_dict(per_class_accuracies, orient='index', columns=['class_accuracy']) \n",
    "\n",
    "                #combine_report_df = pd.concat([per_class_accuracies_df,classification_report_df], axis=1)  (not used)              \n",
    "                \n",
    "                \n",
    "                done = time.ctime()\n",
    "                done2 = time.time()\n",
    "                duration = done2-start2           \n",
    "                print('End time of loop: ', done)\n",
    "                to_append = [model_type, epoch, learning_rate, batch_size, resample_ratio, start, done, duration, accuracy, per_class_accuracies_df, report]\n",
    "                df_length = len(output_df)\n",
    "                output_df.loc[df_length] = to_append\n",
    "                filename_prefix = \"./ner_result/\" + model_type + \"_\" + str(epoch) + \"_\" + str(batch_size)\\\n",
    "                                   + \"_\" + str(learning_rate) + \"_\" + str(resample_ratio) + \"_\" + dataset_name + \"_\" + str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
    "                filename = \"%s.csv\" % filename_prefix\n",
    "                output_df.to_csv(filename, header=True)\n",
    "                print(50*'-')\n",
    "                print(\"<<<Model Performance saved!>>>\")\n",
    "                print(50*'-')\n",
    "                print(50*'-')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4f17f",
   "metadata": {},
   "source": [
    "## ------------------- END OF TRAINING  --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8715a7",
   "metadata": {},
   "source": [
    "## ------------------- START OF HYPERPARAMETER TUNING  --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b167b6",
   "metadata": {},
   "source": [
    "## change to code if you need to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0828fb",
   "metadata": {},
   "source": [
    "For the training set with the resampling ratio combination (oversampling and undersampling) that yields the best classification performance measured on the test dataset, that training dataset can be used for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35590bd9",
   "metadata": {},
   "source": [
    "# specify the training set to use\n",
    "input_resample_params = ['u0.3o1']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85d97509",
   "metadata": {},
   "source": [
    "# Setting range of hyperparameters to be tested for the grid search\n",
    "#h_epoch = [5]\n",
    "#h_batchsize = [16,24]\n",
    "#h_learning_rate = [0.001,0.005,0.0001,0.0005,0.00001]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4841bd38",
   "metadata": {},
   "source": [
    "# Create Output Dataframe to store the performance metrics\n",
    "output_df = pd.DataFrame(columns = ['model', 'epoch', 'learning_rate', 'batch_size', 'resampling_ratio', 'start_time', 'end_time', 'duration', 'overall_accuracy','class_accuracy', 'classification_report'])\n",
    "\n",
    "# test conll data\n",
    "test_data = CoNLL().readDataset(spark, test_file)\n",
    "\n",
    "for resample_ratio in input_resample_params:\n",
    "    train_file = train_folder+\"\\\\\"+dataset_name + \"_\"+str(resample_ratio) + \".txt\"\n",
    "    \n",
    "    ###### Data Preparation\n",
    "    training_data = CoNLL().readDataset(spark, train_file) \n",
    "    #(training_data, test_data) = conll_data.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "    print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "    print(\"Test Dataset Count: \" + str(test_data.count()))\n",
    "\n",
    "    # Groupby view of training data\n",
    "    training_data.select(F.explode(F.arrays_zip('token.result','label.result')).alias(\"cols\")) \\\n",
    "    .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "            F.expr(\"cols['1']\").alias(\"ground_truth\")).groupBy('ground_truth').count().orderBy('count', ascending=False)\\\n",
    "            .show(100,truncate=False)\n",
    "\n",
    "    # Groupby view of testing data\n",
    "    test_data.select(F.explode(F.arrays_zip('token.result','label.result')).alias(\"cols\")) \\\n",
    "    .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "            F.expr(\"cols['1']\").alias(\"ground_truth\")).groupBy('ground_truth').count().orderBy('count', ascending=False)\\\n",
    "            .show(100,truncate=False)\n",
    "    \n",
    "    ## Clinical Embeddings (with Save Model)\n",
    "    ## original n=50, x=8\n",
    "\n",
    "    for n in h_epoch:\n",
    "        for x in h_batchsize:\n",
    "            for z in h_learning_rate:\n",
    "                epoch = n\n",
    "                batch_size = x\n",
    "                learning_rate = z\n",
    "                resample_ratio = resample_ratio\n",
    "                model_type = \"clinical_embeddings\"\n",
    "                start = time.ctime()\n",
    "                start2 = time.time()\n",
    "                print('Start of loop: ', 'epoch =', n, ', batch_size =', x, ', learning_rate =', z, ', resample_ratio =', resample_ratio)\n",
    "                print('Start time for new loop: ', start)\n",
    "                print(50*'-')\n",
    "                #embeddings_clinical_local_path = clinical_model_local_path+\"\\\\embeddings_clinical_en_2.4.0_2.4_1580237286004\"\n",
    "                clinical_embeddings = WordEmbeddingsModel.load(embeddings_clinical_local_path)\\\n",
    "                          .setInputCols([\"sentence\", \"token\"])\\\n",
    "                          .setOutputCol(\"embeddings\")\n",
    "\n",
    "                filename_prefix = \"ner_dl_ncctest_\"  + model_type + str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
    "                clinical_embeddings.transform(test_data).write.parquet(\"%s.parquet\" % filename_prefix)\n",
    "\n",
    "                nerTagger = MedicalNerApproach()\\\n",
    "                            .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "                            .setLabelColumn(\"label\")\\\n",
    "                            .setOutputCol(\"ner\")\\\n",
    "                            .setMaxEpochs(n)\\\n",
    "                            .setLr(z)\\\n",
    "                            .setBatchSize(x)\\\n",
    "                            .setRandomSeed(100)\\\n",
    "                            .setVerbose(1)\\\n",
    "                            .setValidationSplit(0.1)\\\n",
    "                            .setEvaluationLogExtended(True) \\\n",
    "                            .setEnableOutputLogs(True)\\\n",
    "                            .setIncludeConfidence(True)\\\n",
    "                            .setTestDataset(\"%s.parquet\" % filename_prefix)\\\n",
    "                            .setOutputLogsPath('./ner_logs_ncc')\\\n",
    "                            .setGraphFolder('./graph/medical_ner_graphs')\\\n",
    "                            .setEarlyStoppingCriterion(0.01)\\\n",
    "                            .setEarlyStoppingPatience(3)                             \n",
    "                ner_pipeline = Pipeline(stages=[\n",
    "                                clinical_embeddings,\n",
    "                                nerTagger])\n",
    "                ner_model_clinicalembeddings = ner_pipeline.fit(training_data)\n",
    "                test_data = clinical_embeddings.transform(test_data)\n",
    "                predictions = ner_model_clinicalembeddings.transform(test_data)\n",
    "\n",
    "                #===================================================\n",
    "                ## Save NER Model to be used in Pipeline ##done\n",
    "                filename_save = \"./saved_models_tuned/\"+ model_type + \"_\"  + str(epoch) + \"_\" + str(batch_size)\\\n",
    "                                   + \"_\" + str(learning_rate) + \"_\" + str(resample_ratio) + \"_\" + dataset_name\n",
    "\n",
    "                ner_model_clinicalembeddings.stages[1].write().overwrite().save(filename_save)               \n",
    "                #====================================================\n",
    "                \n",
    "                ## Output model predictions to csv file\n",
    "                filename_prefix =  \"./ner_output/\"+ model_type + \"_\"  + str(epoch) + \"_\" + str(batch_size)\\\n",
    "                                   + \"_\" + str(learning_rate) + \"_\" + str(resample_ratio) + \"_\" + dataset_name\n",
    "                filename = \"%s.csv\" % filename_prefix\n",
    "\n",
    "                predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\"))\\\n",
    "                       .select(F.expr(\"cols['0']\").alias(\"token\"), F.expr(\"cols['1']\").alias(\"ground_truth\"), F.expr(\"cols['2']\")\\\n",
    "                       .alias(\"prediction\")).toPandas().to_csv(filename)\n",
    "\n",
    "                preds_df =  predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\"))\\\n",
    "                       .select(F.expr(\"cols['0']\").alias(\"token\"), F.expr(\"cols['1']\").alias(\"ground_truth\"), F.expr(\"cols['2']\")\\\n",
    "                       .alias(\"prediction\")).toPandas()\n",
    "\n",
    "                report = classification_report(preds_df['ground_truth'], preds_df['prediction'], digits=4)\n",
    "                accuracy = accuracy_score(preds_df['ground_truth'] , preds_df['prediction'])\n",
    "                \n",
    "                # get per class accuracy\n",
    "                # https://stackoverflow.com/questions/39770376/scikit-learn-get-accuracy-scores-for-each-class\n",
    "                classes = np.unique(preds_df['ground_truth'])\n",
    "                cm = confusion_matrix(preds_df['ground_truth'], preds_df['prediction']) \n",
    "\n",
    "                # We will store the results in a dictionary for easy access later\n",
    "                per_class_accuracies = {}\n",
    "\n",
    "                # Calculate the accuracy for each one of our classes\n",
    "                for idx, cls in enumerate(classes):\n",
    "                    # True negatives are all the samples that are not our current GT class (not the current row) \n",
    "                    # and were not predicted as the current class (not the current column)\n",
    "                    true_negatives = np.sum(np.delete(np.delete(cm, idx, axis=0), idx, axis=1))\n",
    "\n",
    "                    # True positives are all the samples of our current GT class that were predicted as such\n",
    "                    true_positives = cm[idx, idx]\n",
    "                    \n",
    "                    # 03-jul-2023: dont consider TN, use TP/(TP+FP+FN), same formulae for whole manuscript\n",
    "                    per_class_accuracies[cls] = (true_positives) / (np.sum(cm)-true_negatives)                    \n",
    "\n",
    "                # Combine class accuracies to classification report\n",
    "                report_dict = classification_report(preds_df['ground_truth'], preds_df['prediction'], digits=4, labels=classes, output_dict=True)\n",
    "                classification_report_df = pd.DataFrame(report_dict).transpose()\n",
    "                per_class_accuracies_df = pd.DataFrame.from_dict(per_class_accuracies, orient='index', columns=['class_accuracy']) \n",
    "                #combine_report_df = pd.concat([per_class_accuracies_df,classification_report_df], axis=1)   (not used)                                           \n",
    "                done = time.ctime()\n",
    "                done2 = time.time()\n",
    "                duration = done2-start2           \n",
    "                print('End time of loop: ', done)\n",
    "                to_append = [model_type, epoch, learning_rate, batch_size, resample_ratio, start, done, duration, accuracy, per_class_accuracies_df, report]\n",
    "                df_length = len(output_df)\n",
    "                output_df.loc[df_length] = to_append\n",
    "                filename_prefix = \"./ner_result/\" + model_type + \"_\" + str(epoch) + \"_\" + str(batch_size)\\\n",
    "                                   + \"_\" + str(learning_rate) + \"_\" + str(resample_ratio) + \"_\" + dataset_name + \"_\" + str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
    "                filename = \"%s.csv\" % filename_prefix\n",
    "                output_df.to_csv(filename, header=True)\n",
    "                print(50*'-')\n",
    "                print(\"<<<Model Performance saved!>>>\")\n",
    "                print(50*'-')\n",
    "                print(50*'-')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b898f",
   "metadata": {},
   "source": [
    "### Generate Validation and Testing Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d899d",
   "metadata": {},
   "source": [
    "Note: Each training model run will generate a log file, saved at .\\ner_log_ncc\\\n",
    "This log file can be used as an input to Spark NLP's ner_log_parser.get_charts function to generate the validation and testing learning curves"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ef438a9",
   "metadata": {},
   "source": [
    "ner_log_parser.get_charts('./ner_logs_ncc/MedicalNerApproach_986eec60057e.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6b9f6",
   "metadata": {},
   "source": [
    "## ------------------- END OF HYPERPARAMETER TUNING  --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12acd1",
   "metadata": {},
   "source": [
    "## ------------------- MODEL INFERENCE --------------------\n",
    "same code as 03predict_radio_ner_v1.0.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a515df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the best ner model you want to use for prediction\n",
    "# this is saved during NER training\n",
    "best_ner_model = \"clinical_embeddings_1_8_0.001_u0.3o1_train4522\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d80e8",
   "metadata": {},
   "source": [
    "## Prediction Pipeline - with jsl assertion status detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "document = DocumentAssembler()\\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "        .setInputCols(['document'])\\\n",
    "        .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "        .setInputCols(['sentence'])\\\n",
    "        .setOutputCol('token')\n",
    "\n",
    "#use .pretrained() for sparknlp online session\n",
    "#use .load() for sparknlp airgap session\n",
    "clinical_embeddings = WordEmbeddingsModel.load(embeddings_clinical_local_path)\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "#load the best ner model saved after training\n",
    "loaded_ner_model = MedicalNerModel.load(\"./saved_models/\" + best_ner_model)\\\n",
    "        .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "        .setOutputCol(\"ner\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "        .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
    "        .setOutputCol(\"ner_span\")\n",
    "\n",
    "#using jsl radiology assertion model for all ner entities \n",
    "#use .pretrained() for sparknlp online session\n",
    "#use .load() for sparknlp airgap session\n",
    "#radiology_assertion = AssertionDLModel.pretrained(\"assertion_dl_radiology\", \"en\", \"clinical/models\") \\\n",
    "radiology_assertion = AssertionDLModel.load(\"./assertion_dl_radiology_en_2.7.4_2.4_1616071311532\")\\\n",
    "    .setInputCols([\"sentence\", \"ner_span\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion\")\n",
    "\n",
    "ner_prediction_pipeline = Pipeline(stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        token,\n",
    "        clinical_embeddings,\n",
    "        loaded_ner_model,\n",
    "        converter,\n",
    "        radiology_assertion\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "\n",
    "ner_prediction_model = ner_prediction_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40403f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c535db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "your sample text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text1\n",
    "sample_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "sample_data.show()\n",
    "sample_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ner_prediction_model.transform(sample_data)\n",
    "\n",
    "preds.select(F.explode(F.arrays_zip('ner_span.result', \n",
    "                                     'ner_span.metadata', \n",
    "                                     'assertion.result')).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
    "              F.expr(\"cols['1']['sentence']\").alias(\"sent_id\"),\n",
    "              F.expr(\"cols['2']\").alias(\"assertion\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400081c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel = LightPipeline(ner_prediction_model)\n",
    "ppres = lmodel.fullAnnotate(text)[0]\n",
    "ppres.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppres['ner_span']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertion detection\n",
    "chunk=[]\n",
    "entity=[]\n",
    "status=[]\n",
    "for n,m in zip(ppres['ner_span'],ppres['assertion']):\n",
    "    chunk.append(n.result)\n",
    "    entity.append(n.metadata['entity']) \n",
    "    status.append(m.result)\n",
    "\n",
    "df = pd.DataFrame({'chunk':chunk, 'entity':entity, 'assertion_status':status})\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b47c2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "visualiser = NerVisualizer()\n",
    "\n",
    "# Set label filter\n",
    "visualiser.display(ppres, label_col='ner_span', document_col='document', save_path=\"./inference/display_result/display_result.html\")\n",
    "df.to_csv(\"./inference/display_result/display_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d1d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89af4f1a",
   "metadata": {},
   "source": [
    "## Get prediction with input csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb71c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the codes accordingly based on your csv file\n",
    "# it should contain a text column \"conclusion\"\n",
    "df_text = pd.read_csv(\"./inference/sample.csv\", usecols=['sn_report_number','report','report_date','conclusion'])\n",
    "df_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null text\n",
    "df_text.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null\n",
    "df_text['conclusion'] = df_text['conclusion'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the ner visualisation to html file for review\n",
    "# save the ner annotation to csv for review\n",
    "\n",
    "annotation_df = pd.DataFrame()\n",
    "for i in range(df_text['sn_report_number'].count()):\n",
    "    print(i)\n",
    "    ppres = lmodel.fullAnnotate(df_text['conclusion'].loc[i])[0]\n",
    "    visualiser.display(ppres, label_col='ner_span', document_col='document', save_path=\"./inference/display_result/\"+df_text['sn_report_number'].loc[i]+\"_report.html\")\n",
    "\n",
    "    #output to csv\n",
    "    chunk=[]\n",
    "    entity=[]\n",
    "    status=[]\n",
    "    for n,m in zip(ppres['ner_span'],ppres['assertion']):\n",
    "        chunk.append(n.result)\n",
    "        entity.append(n.metadata['entity']) \n",
    "        status.append(m.result)\n",
    "    temp_df = pd.DataFrame({'sn_report_number':df_text['sn_report_number'].loc[i],'report_date':df_text['report_date'].loc[i],'chunk':chunk, 'entity':entity, 'assertion_status':status})    \n",
    "    temp_df['entity_index'] = temp_df.index\n",
    "    #print(temp_df)\n",
    "    annotation_df = annotation_df.append(temp_df)\n",
    "    #print(annotation_df)\n",
    "\n",
    "columns = ['sn_report_number', 'report_date','entity_index', 'entity','chunk','assertion_status']\n",
    "annotation_df.to_csv(\"./inference/display_result/pred_radio_ner_annotation.csv\", columns=columns, index=False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
