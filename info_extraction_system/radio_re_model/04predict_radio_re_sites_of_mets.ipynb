{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7de342",
   "metadata": {},
   "source": [
    "## Purpose: single Prediction Pipeline NER + Assertion Detection + RE_SITES_OF_METS + post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3589671",
   "metadata": {},
   "source": [
    "### Note: Before running this notebook, please configure the following paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using sparknlp clinical embedding word model\n",
    "# specify your folder containing the downloaded clinical embedding word model file, or you can use .pretrained during training instead to load it online\n",
    "embeddings_clinical_local_path = r\"path\\to\\sparknlp_pretrained\\embeddings_clinical_en_2.4.0_2.4_1580237286004\"\n",
    "model_type = \"clinical_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6808f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your sparknlp online license key-need internet connection\n",
    "# we are using v3.4.2\n",
    "sparknlp_licence_key = r\"..\\sparknlp_licence_key\\yourkey.json\"\n",
    "\n",
    "# specify your sparknlp offline license key-airgap env\n",
    "# we are using v3.4.2\n",
    "sparknlp_airgap_licence_key = r\"..\\sparknlp_licence_key\\yourairgapkey.json\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d28bcf0",
   "metadata": {},
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1203f2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, sparknlp, sparknlp_jsl, datetime, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.training import CoNLL\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp_jsl.training import tf_graph\n",
    "from sparknlp_display import AssertionVisualizer, NerVisualizer,RelationExtractionVisualizer \n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118e80f",
   "metadata": {},
   "source": [
    "## Start Spark Session (offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766fff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline-Load airgap license key\n",
    "with open(sparknlp_airgap_licence_key) as f:\n",
    "    airgap_license_keys = json.load(f)\n",
    "    \n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(airgap_license_keys)\n",
    "os.environ.update(airgap_license_keys)\n",
    "\n",
    "# check variable\n",
    "!echo $SECRET\n",
    "!echo $JSL_VERSION\n",
    "!echo $PUBLIC_VERSION\n",
    "\n",
    "# to resolve sparknlp module not found during re training, set the following env\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])\n",
    "\n",
    "# Start Spark Session with Custom Params (OFFLINE)\n",
    "# https://spark.apache.org/docs/latest/configuration.html#memory-management\n",
    "# Important! memory setting need to be adjusted for different work load \n",
    "\n",
    "def start(SECRET):\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed radio mets jupyter\") \\\n",
    "        .master(\"local[16]\") \\\n",
    "        .config(\"spark.driver.memory\", \"30G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.driver.maxResultSize\",\"8000M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\") \\\n",
    "        .config(\"spark.jars\", f\"d:\\content\\spark-nlp-jsl-{JSL_VERSION}.jar, d:\\airgap\\spark-nlp_2.12-3.4.2.jar\" )\n",
    "\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = start(SECRET) \n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137786a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "935759f0",
   "metadata": {},
   "source": [
    "## Start Spark Session (online)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a663c",
   "metadata": {},
   "source": [
    "Note: Requires Spark NLP and Spark NLP for Healthcare (licensed version) packages to be installed"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2bab3ae",
   "metadata": {},
   "source": [
    "# NEW, use this code, no need to define env variables\n",
    "# Load license key\n",
    "with open(sparknlp_licence_key) as f:\n",
    "    license_keys = json.load(f)\n",
    "    \n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "\n",
    "# Adding license key-value pairs to environment variables, use this (17-Aug-2022)\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "\n",
    "# check variable\n",
    "!echo $SECRET\n",
    "!echo $JSL_VERSION\n",
    "!echo $PUBLIC_VERSION\n",
    "\n",
    "# to resolve sparknlp module not found during re training, set the following env\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b201e5ec",
   "metadata": {},
   "source": [
    "# https://spark.apache.org/docs/latest/configuration.html#memory-management\n",
    "# Important! memory setting need to be adjusted for different work load \n",
    "\n",
    "# Start Spark Session\n",
    "params = {\"spark.master\":\"local[32]\",\n",
    "          \"spark.driver.memory\":\"80G\", \n",
    "          \"spark.kryoserializer.buffer.max\":\"2000M\", \n",
    "          \"spark.driver.maxResultSize\":\"8000M\",\n",
    "          \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "          \"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\",\n",
    "          \"spark.jars\": \"https://pypi.johnsnowlabs.com/\"+SECRET+\"/spark-nlp-jsl-\"+JSL_VERSION+\".jar\",\n",
    "          \"spark.local.dir\": r\"c:\\users\\guathwa\\spark-temp\"\n",
    "           } \n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257defd",
   "metadata": {},
   "source": [
    "## ------------------- MODEL INFERENCE --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a18bd4",
   "metadata": {},
   "source": [
    "### Test single Prediction Pipeline NER + Assertion Detection + RE_SITES_OF_METS\n",
    "copy the radio NER model and radio cancer assertion model to be used for this pipeline in radio_re_model/sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a716559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the name of NER model\n",
    "radio_ner_model = \"clinical_embeddings_5_8_0.001_u0.4o1_train4522\"\n",
    "\n",
    "# specify the name of assertion model\n",
    "radio_assertion_model = \"radio_assertion_model_10_16_0.001_2022_11_18_15_39_34\"\n",
    "\n",
    "# specify the name of re_mets_model_name, in saved_models folder\n",
    "re_mets_model_name = \"re_sites_of_mets_50_16_0.005_2023_04_21_11_05_40_train4522\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e59e6d",
   "metadata": {},
   "source": [
    "### Load trained model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_relations_df (results, col='relations'):\n",
    "  rel_pairs=[]\n",
    "  for rel in results[0][col]:\n",
    "      rel_pairs.append((\n",
    "          rel.result, \n",
    "          rel.metadata['entity1'], \n",
    "          rel.metadata['entity1_begin'],\n",
    "          rel.metadata['entity1_end'],\n",
    "          rel.metadata['chunk1'], \n",
    "          rel.metadata['entity2'],\n",
    "          rel.metadata['entity2_begin'],\n",
    "          rel.metadata['entity2_end'],\n",
    "          rel.metadata['chunk2'], \n",
    "          rel.metadata['confidence']\n",
    "      ))\n",
    "\n",
    "  rel_df = pd.DataFrame(rel_pairs, columns=['relation','entity1','entity1_begin','entity1_end','chunk1','entity2','entity2_begin','entity2_end','chunk2', 'confidence'])\n",
    "\n",
    "  return rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e53201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common block\n",
    "# 11-Aug-2022 add chunk filterer to filter entities of interest\n",
    "documenter = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentencer = SentenceDetector()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentences\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"sentences\"])\\\n",
    "    .setOutputCol(\"tokens\")\\\n",
    "\n",
    "words_embedder = WordEmbeddingsModel()\\\n",
    "    .load(embeddings_clinical_local_path)\\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "pos_tagger = PerceptronModel()\\\n",
    "    .load(\"./pretrained/pos_clinical_en_3.0.0_3.0_1617052315327\") \\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"pos_tags\")\n",
    "\n",
    "dependency_parser = DependencyParserModel()\\\n",
    "    .load(\"./pretrained/dependency_conllu_en_3.0.0_3.0_1656858083101\")\\\n",
    "    .setInputCols([\"sentences\", \"pos_tags\", \"tokens\"])\\\n",
    "    .setOutputCol(\"dependencies\")\n",
    "\n",
    "# to detect all radio ner\n",
    "radio_ner = MedicalNerModel.load(radio_ner_model)\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"ner_tags\")\n",
    "\n",
    "# to get all radio ner chunks\n",
    "radio_ner_converter = NerConverter()\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "    .setOutputCol(\"ner_chunks\")\n",
    "\n",
    "# to filter for required ners\n",
    "non_cancer_ner_converter = NerConverter()\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "    .setOutputCol(\"non_cancer_ner_chunks\")\\\n",
    "    .setWhiteList([\"body_part\",\"anatomical_descriptor\",\"direction\",\"probability_high\",\"probability_medium\",\"probability_uncertain\",\"probability_low\"])\n",
    "\n",
    "## to filter for cancer_imaging_findings ner chunks, and send for assertion detection\n",
    "cancer_ner_converter = NerConverter()\\\n",
    "        .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "        .setOutputCol(\"cancer_ner_chunks\")\\\n",
    "        .setWhiteList([\"cancer_imaging_findings\"])\n",
    "\n",
    "## assertion detection for cancer_imaging_findings\n",
    "cancer_ner_assertion = AssertionDLModel.load(radio_assertion_model) \\\n",
    "    .setInputCols([\"sentences\", \"cancer_ner_chunks\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"cancer_assertion\")\n",
    "\n",
    "## add filterer to filter for probability_high/probability_medium\n",
    "cancer_ner_assertion_filterer = AssertionFilterer()\\\n",
    "    .setInputCols(\"sentences\",\"cancer_ner_chunks\",\"cancer_assertion\")\\\n",
    "    .setOutputCol(\"cancer_assertion_filtered\")\\\n",
    "    .setWhiteList([\"probability_high\",\"probability_medium\"])\n",
    "\n",
    "# merge ner_chunks by prioritizing the overlapping indices (chunks with longer lengths and highest information will be kept from each ner model)\n",
    "chunk_merger = ChunkMergeApproach()\\\n",
    "    .setInputCols('non_cancer_ner_chunks', \"cancer_assertion_filtered\")\\\n",
    "    .setOutputCol('merged_ner_chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf777c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to set the entity pairs you need for this prediction pipeline\n",
    "pairs_mets = ['cancer_imaging_findings-body_part', \\\n",
    "            'cancer_imaging_findings-anatomical_descriptor', \\\n",
    "            'anatomical_descriptor-body_part', \\\n",
    "            'direction-body_part',\n",
    "            'direction-anatomical_descriptor'\n",
    "            ]             \n",
    "\n",
    "mets_rel_pairs = pairs_mets\n",
    "\n",
    "##============================================================================\n",
    "loaded_re_mets_model = RelationExtractionModel()\\\n",
    "    .load('./saved_models/'+re_mets_model_name)\\\n",
    "    .setInputCols([\"embeddings\", \"pos_tags\", \"merged_ner_chunks\", \"dependencies\"])\\\n",
    "    .setOutputCol(\"mets_relations\")\\\n",
    "    .setRelationPairs(mets_rel_pairs)\\\n",
    "    .setMaxSyntacticDistance(4)\\\n",
    "    .setPredictionThreshold(0.8)\n",
    "\n",
    "re_mets_pipeline = Pipeline(stages=[\n",
    "    documenter,\n",
    "    sentencer,\n",
    "    tokenizer, \n",
    "    words_embedder, \n",
    "    pos_tagger, \n",
    "    dependency_parser,\n",
    "    radio_ner,\n",
    "    radio_ner_converter,\n",
    "    non_cancer_ner_converter,\n",
    "    cancer_ner_converter,\n",
    "    cancer_ner_assertion,\n",
    "    cancer_ner_assertion_filterer,\n",
    "    chunk_merger,\n",
    "    loaded_re_mets_model\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "re_mets_pipeline_model = re_mets_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# site of mets\n",
    "mtext1 = \"\"\"\n",
    "your sample text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = mtext1\n",
    "sample_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "sample_data.show()\n",
    "sample_data.dtypes\n",
    "\n",
    "model = re_mets_pipeline_model\n",
    "#model = re_mets_merge_pipeline_model\n",
    "preds = model.transform(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cancer assertion status\n",
    "preds.select(F.explode(F.arrays_zip(preds.cancer_ner_chunks.result, \n",
    "                                     preds.cancer_ner_chunks.metadata, \n",
    "                                     preds.cancer_assertion.result)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunks\"),\n",
    "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
    "              F.expr(\"cols['1']['sentences']\").alias(\"sent_id\"),\n",
    "              F.expr(\"cols['2']\").alias(\"assertion\")).show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba896e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cancer assertion filterer\n",
    "preds.select(F.explode(F.arrays_zip(preds.cancer_ner_chunks.result, \n",
    "                                     preds.cancer_ner_chunks.metadata, \n",
    "                                     preds.cancer_assertion_filtered.result)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunks\"),\n",
    "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
    "              F.expr(\"cols['1']['sentences']\").alias(\"sent_id\"),\n",
    "              F.expr(\"cols['2']\").alias(\"filtered_assertion\")).show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fea8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check merged_ner_chunks\n",
    "preds.select(F.explode(F.arrays_zip(preds.merged_ner_chunks.result, \n",
    "                                     preds.merged_ner_chunks.metadata)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunks\"),\n",
    "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
    "              F.expr(\"cols['1']['sentences']\").alias(\"sent_id\")).show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33deebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = preds.select(F.explode(F.arrays_zip('mets_relations.result', 'mets_relations.metadata')).alias(\"cols\")) \\\n",
    ".select(F.expr(\"cols['0']\").alias(\"relations\"),\n",
    "        F.expr(\"cols['1']['sentence']\").alias(\"sentence_id\"),\n",
    "        F.expr(\"cols['1']['entity1']\").alias(\"entity1\"),\n",
    "#        F.expr(\"cols['1']['entity1_begin']\").alias(\"entity1_begin\"),\n",
    "#        F.expr(\"cols['1']['entity1_end']\").alias(\"entity1_end\"),\n",
    "        F.expr(\"cols['1']['chunk1']\").alias(\"chunk1\"),\n",
    "        F.expr(\"cols['1']['entity2']\").alias(\"entity2\"),\n",
    "#        F.expr(\"cols['1']['entity2_begin']\").alias(\"entity2_begin\"),\n",
    "#        F.expr(\"cols['1']['entity2_end']\").alias(\"entity2_end\"),\n",
    "        F.expr(\"cols['1']['chunk2']\").alias(\"chunk2\"),\n",
    "        F.expr(\"cols['1']['confidence']\").alias(\"confidence\")\n",
    "        )\n",
    "\n",
    "result_df.show(n=10, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e60449",
   "metadata": {},
   "source": [
    "### Create a light pipeline for annotating free text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c46eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = re_mets_pipeline_model\n",
    "light_model = LightPipeline(model)\n",
    "\n",
    "annotations = light_model.fullAnnotate(text)\n",
    "\n",
    "rel_df = get_relations_df(annotations,\"mets_relations\")\n",
    "rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da019e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis= RelationExtractionVisualizer()\n",
    "vis.display(annotations[0], 'mets_relations', show_relations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bffaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f9c030b",
   "metadata": {},
   "source": [
    "## Get prediction with sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902472fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_csv(\"./inference/sample.csv\", usecols=['sn_report_number','Report','report_date','Conclusion'])\n",
    "df_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44adb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null text\n",
    "df_text.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null\n",
    "df_text['Conclusion'] = df_text['Conclusion'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade18756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the re visualisation to html file for review\n",
    "# save the re annotation to csv for review\n",
    "for i in range(df_text['sn_report_number'].count()):\n",
    "    text = df_text['Conclusion'].loc[i]\n",
    "    annotations = light_model.fullAnnotate(text)\n",
    "    temp_df = get_relations_df(annotations,'mets_relations')\n",
    "    print(temp_df)\n",
    "    # write to csv\n",
    "    temp_df.to_csv('./inference/display_result/'+df_text['sn_report_number'].iloc[i]+'_tabular.csv')\n",
    "    vis.display(annotations[0], 'mets_relations', show_relations=True, save_path=\"./inference/display_result/\"+df_text['sn_report_number'].loc[i]+\"_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcbf76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b93ce83d",
   "metadata": {},
   "source": [
    "## Specify input file to get prediction (testset 460 reports for manuscript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e675359",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(\"./inference/testset_460.csv\")\n",
    "input_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdf70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"conclusion\"] = input_df[\"conclusion\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e9841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50475d4c",
   "metadata": {},
   "source": [
    "## Post Processing to derive site of mets (term normalization)\n",
    "mapping files:\n",
    "- 01_map_anatomical_descriptor_to_body_part.csv\n",
    "- 02_map_normalized_body_part.csv\n",
    "- 03_map_cancer_body_part.csv\n",
    "- 04_map_merge_body_part.csv\n",
    "- 05_map_drop_body_part.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75da704",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"./mapping/01_map_anatomical_descriptor_to_body_part.csv\")\n",
    "# to convert all to lower case \n",
    "temp_df.anatomical_descriptor = temp_df.anatomical_descriptor.str.lower().str.rstrip()\n",
    "temp_df.map_to = temp_df.map_to.str.lower().str.rstrip()\n",
    "\n",
    "map_anat_descriptor_dict = dict(zip(list(temp_df.anatomical_descriptor), list(temp_df.map_to)))\n",
    "#map_anat_descriptor_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"./mapping/03_map_normalized_body_part.csv\")\n",
    "# to convert all to lower case \n",
    "temp_df.body_part = temp_df.body_part.str.lower().str.rstrip()\n",
    "temp_df.normalized_body_part = temp_df.normalized_body_part.str.lower()\n",
    "map_nbody_part_dict = dict(zip(list(temp_df.body_part), list(temp_df.normalized_body_part)))\n",
    "#map_nbody_part_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439407a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbody_part_df = pd.read_csv(\"./mapping/04_map_merge_body_part.csv\")\n",
    "# to convert all to lower case \n",
    "mbody_part_df.body_part1 = mbody_part_df.body_part1.str.lower().str.rstrip()\n",
    "mbody_part_df.body_part2 = mbody_part_df.body_part2.str.lower().str.rstrip()\n",
    "mbody_part_df.output_body_part = mbody_part_df.output_body_part.str.lower().str.rstrip()\n",
    "mbody_part_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_cancer_chunk = pd.read_csv(\"./mapping/02_map_cancer_body_part_anat.csv\")\n",
    "# to convert all to lower case \n",
    "map_cancer_chunk.cancer_imaging_findings = map_cancer_chunk.cancer_imaging_findings.str.lower().str.rstrip()\n",
    "map_cancer_chunk.body_part_anat = map_cancer_chunk.body_part_anat.str.lower().str.rstrip()\n",
    "map_cancer_chunk = map_cancer_chunk.drop_duplicates()\n",
    "map_cancer_chunk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_to_drop = pd.read_csv(\"./mapping/05_map_drop_body_part.csv\")\n",
    "# to convert all to lower case \n",
    "map_to_drop.body_part_to_drop = map_to_drop.body_part_to_drop.str.lower().str.rstrip()\n",
    "map_to_drop = map_to_drop.drop_duplicates()\n",
    "map_to_drop = set(map_to_drop.body_part_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da7ed4",
   "metadata": {},
   "source": [
    "## 04-Apr-2023 New post processing logic based on dataframe approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e79f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping02_bpart(cancer,bpart, df_mapping02):\n",
    "    \n",
    "    # find mapping for the input cancer chunk\n",
    "    df_cancer_map = df_mapping02[df_mapping02[\"cancer_imaging_findings\"]==cancer]\n",
    "    #print(df_cancer_map)\n",
    "    \n",
    "    # check if cancer chunk is found in mapping file.\n",
    "    # if yes, search and get the output_body_part\n",
    "    if len(df_cancer_map) > 0:\n",
    "        \n",
    "        for idx,row in df_cancer_map.iterrows():\n",
    "\n",
    "            # convert string to list\n",
    "            map_list = row[\"body_part_anat\"].split(',')\n",
    "            map_list = [x.strip() for x in map_list]\n",
    "            #print(map_list)\n",
    "\n",
    "            # search bdpart in the list of values. if found, return the output_body_part\n",
    "            # if not found, return \"\"\n",
    "\n",
    "            if bpart in map_list:\n",
    "\n",
    "                # 22-mar-2023 handle no value in output_body_part (eg cake + omental return no value)\n",
    "                if row[\"output_body_part\"] != \"\":\n",
    "                    print(\"***map \",row[\"cancer_imaging_findings\"],\"+\",bpart,\"to \",row[\"output_body_part\"])               \n",
    "                    return(row[\"output_body_part\"])\n",
    "                else:\n",
    "                    print(\"***map \",row[\"cancer_imaging_findings\"],\"+\",bpart,\"to no value\")  \n",
    "                    return(\"\")\n",
    "            else:\n",
    "                # bdpart not found in list, check new row\n",
    "                #print(cancer + \"+\" + bpart + \" not found in mapping 02\")\n",
    "                #return(bpart)\n",
    "                pass\n",
    "        # after processing all available rows, still not found\n",
    "        print(cancer + \"+\" + bpart + \" not found in mapping 02\")\n",
    "        return(bpart)        \n",
    "    else:\n",
    "        print(cancer + \" not found in mapping 02\")\n",
    "        # return original body_part\n",
    "        return(bpart)\n",
    "    \n",
    "df_mapping02 = map_cancer_chunk    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine code to run in flask\n",
    "mets_df = input_df.copy()\n",
    "mets_df[\"pred_site_of_mets\"]=\"\"\n",
    "# check 244 294 289\n",
    "#for i in range(40,41):\n",
    "#for i in [34,56,63,64,67,114,127,156,204,209,211,225,408,409,438,443,452,453,454]:\n",
    "for i in range(input_df['conclusion'].count()):\n",
    "   \n",
    "    print(\"row: \", i)\n",
    "    \n",
    "    # clean text\n",
    "    text = input_df['conclusion'].loc[i].lower()\n",
    "    print(text)\n",
    "\n",
    "    # get predictions\n",
    "    annotations = light_model.fullAnnotate(text)\n",
    "    rel_df = get_relations_df(annotations,\"mets_relations\")\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 1 - get relation: cancer_imaging_findings > body_part (both direction), append to df\n",
    "    # columns cancer_imaging_findings body_part\n",
    "    ###############################################################################################    \n",
    "    # get cancer_imaging_findings - body_part (both direction)\n",
    "    print(\"\\n-------------------- step 1 --------------------\")\n",
    "    print(\"***get cancer to body_part\\n\")\n",
    "    pair1 = rel_df[(rel_df['entity1']=='cancer_imaging_findings') & (rel_df['entity2']=='body_part')][['chunk1','chunk2']]\n",
    "    pair2 = rel_df[(rel_df['entity2']=='cancer_imaging_findings') & (rel_df['entity1']=='body_part')][['chunk2','chunk1']]\n",
    "    pair2 = pair2.rename(columns={\"chunk2\":\"chunk1\", \"chunk1\":\"chunk2\"})\n",
    "    df_cancer_bpart = pd.concat([pair1,pair2])\n",
    "    df_cancer_bpart = df_cancer_bpart.rename(columns={\"chunk1\":\"cancer_imaging_findings\", \"chunk2\":\"body_part\"})\n",
    "    print(df_cancer_bpart)\n",
    "\n",
    "    # check for lymphangitis carcinomatosis, add pulmonary\n",
    "    s = \"lymphangitis carcinomatosis\"\n",
    "    if s in rel_df[rel_df['entity1']=='cancer_imaging_findings']['chunk1'].str.lower().values.tolist():\n",
    "        print(\"***found \",s, \"add pulmonary\")\n",
    "        to_append = [s,\"pulmonary\"]\n",
    "        df_length = len(df_cancer_bpart)\n",
    "        df_cancer_bpart.loc[df_length] = to_append    \n",
    "    elif s in rel_df[rel_df['entity2']=='cancer_imaging_findings']['chunk2'].str.lower().values.tolist():\n",
    "        print(\"***found \",s, \"add pulmonary\")\n",
    "        to_append = [s,\"pulmonary\"]\n",
    "        df_length = len(df_cancer_bpart)\n",
    "        df_cancer_bpart.loc[df_length] = to_append   \n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 2a - get relation: cancer_imaging_findings > anatomical_descriptor (both direction)\n",
    "    # columns cancer_imaging_findings anatomical_descriptor body_part\n",
    "    ###############################################################################################\n",
    "    print(\"\\n-------------------- step 2a --------------------\")   \n",
    "    print(\"***get cancer to anatomical_descriptor\\n\")\n",
    "    # get cancer_imaging_findings - anatomical_descriptor (need begin/end to link to bodypart) (both direction)\n",
    "    pair1 = rel_df[(rel_df['entity1']=='cancer_imaging_findings') & (rel_df['entity2']=='anatomical_descriptor')][['chunk1','chunk2','entity2_begin','entity2_end']]\n",
    "    pair2 = rel_df[(rel_df['entity2']=='cancer_imaging_findings') & (rel_df['entity1']=='anatomical_descriptor')][['chunk2','chunk1','entity1_begin','entity1_end']]\n",
    "    pair2 = pair2.rename(columns={\"chunk2\":\"chunk1\", \"chunk1\":\"chunk2\",\"entity1_begin\":\"entity2_begin\",\"entity1_end\":\"entity2_end\"})\n",
    "    df_cancer_anat = pd.concat([pair1,pair2])\n",
    "    df_cancer_anat = df_cancer_anat.rename(columns={\"chunk1\":\"cancer_imaging_findings\", \"chunk2\":\"anatomical_descriptor\",\"entity2_begin\":\"anat_begin\",\"entity2_end\":\"anat_end\"})\n",
    "    print(df_cancer_anat)\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 2b - get relation: anatomical descriptor > body_part (both direction)\n",
    "    # columns cancer_imaging_findings anatomical_descriptor anat_begin anat_end\n",
    "    ###############################################################################################\n",
    "    print(\"\\n-------------------- step 2b --------------------\")   \n",
    "    print(\"***get anatomical_descriptor to body_part\\n\") \n",
    "    # get cancer_imaging_findings - anatomical_descriptor (need begin/end to link to bodypart) (both direction)\n",
    "    pair1 = rel_df[(rel_df['entity1']=='anatomical_descriptor') & (rel_df['entity2']=='body_part')][['chunk1','chunk2','entity1_begin','entity1_end']]\n",
    "    pair2 = rel_df[(rel_df['entity2']=='anatomical_descriptor') & (rel_df['entity1']=='body_part')][['chunk2','chunk1','entity2_begin','entity2_end']]\n",
    "    pair2 = pair2.rename(columns={\"chunk2\":\"chunk1\", \"chunk1\":\"chunk2\", \"entity2_begin\":\"entity1_begin\", \"entity2_end\":\"entity1_end\"})\n",
    "    df_anat_bpart = pd.concat([pair1,pair2])\n",
    "    df_anat_bpart = df_anat_bpart.rename(columns={\"chunk1\":\"anatomical_descriptor\", \"entity1_begin\":\"anat_begin\",\"entity1_end\":\"anat_end\",\"chunk2\":\"body_part\"})\n",
    "    print(df_anat_bpart)\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 2c1 - link cancer > anatomical descriptor > body_part (2 hops), get body_part\n",
    "    # link based on anatomatical_descriptor+begin+end\n",
    "    # this is to get cancer > anatomical_descriptor > body_part\n",
    "    # if body_part = NaN, means this anatomical_descriptor has no link to body part, exclude (step 2c2 will process mapping file 01)\n",
    "    ###############################################################################################\n",
    "    print(\"\\n-------------------- step 2c1 --------------------\")      \n",
    "    print(\"***get cancer to anatomical_descriptor to body_part (RE-2 hops)\") \n",
    "    df_cancer_anat_bpart1 = pd.merge(df_cancer_anat,df_anat_bpart,how=\"inner\")\n",
    "    print(df_cancer_anat_bpart1)\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 2c2 - cancer > anatomical descriptor > map body_part (mapping file 01)\n",
    "    ###############################################################################################\n",
    "    print(\"\\n-------------------- step 2c2 --------------------\")      \n",
    "    print(\"***get cancer to anatomical_descriptor to mapped body_part (mapping file 01)\") \n",
    "\n",
    "    # make a copy of df_cancer_anat to store the mapped body_part\n",
    "    df_cancer_anat_bpart2 = df_cancer_anat.copy()\n",
    "    df_cancer_anat_bpart2[\"body_part\"] = \"\"\n",
    "\n",
    "    for idx,row in df_cancer_anat_bpart2.iterrows():\n",
    "        c = row['anatomical_descriptor'].lower()\n",
    "\n",
    "        # check for segment*, add hepatic\n",
    "        #print(\"***checking for segment*: \",c)\n",
    "        if (c.find(\"segment\") != -1) or (c==\"segment\"):\n",
    "            print(\"***found segment*, update body_part to hepatic for segment*\")\n",
    "            df_cancer_anat_bpart2.at[idx,'body_part'] = \"hepatic\"\n",
    "\n",
    "        else:\n",
    "            # look up c in mapping file 01   \n",
    "            try:\n",
    "                v = map_anat_descriptor_dict[c]\n",
    "                print(\"***map \",c,\" -> \",v)\n",
    "                df_cancer_anat_bpart2.at[idx,'body_part'] = v\n",
    "                print(\"row updated\")\n",
    "            except:\n",
    "                pass    \n",
    "\n",
    "    # fill null with \"\", and filter for rows with body_part values \n",
    "    df_cancer_anat_bpart2 = df_cancer_anat_bpart2.fillna(\"\")\n",
    "    df_cancer_anat_bpart2 = df_cancer_anat_bpart2[df_cancer_anat_bpart2[\"body_part\"]!=\"\"]   \n",
    "    print(df_cancer_anat_bpart2)\n",
    "\n",
    "    # resolve conflict between df_cancer_anat_bpart1(via link, prediction may be wrong), df_cancer_anat_bpart2(via map, more accurate)\n",
    "    # logic1-take all from df_cancer_anat_bpart2, union remaining from df_cancer_anat_bpart1\n",
    "    print(\"\\n***resolving conflict between linked body_part / mapped body_part\")\n",
    "    df_cancer_anat_bpart1 = df_cancer_anat_bpart1.rename(columns={\"body_part\":\"linked_body_part\"})\n",
    "    df_cancer_anat_bpart2= df_cancer_anat_bpart2.rename(columns={\"body_part\":\"mapped_body_part\"})\n",
    "    df_step2_cancer_bpart = pd.merge(df_cancer_anat_bpart1,df_cancer_anat_bpart2, how=\"outer\")\n",
    "    df_step2_cancer_bpart[\"body_part\"] = np.where(df_step2_cancer_bpart[\"mapped_body_part\"].notnull(), df_step2_cancer_bpart[\"mapped_body_part\"], df_step2_cancer_bpart[\"linked_body_part\"])\n",
    "    print(df_step2_cancer_bpart)\n",
    "\n",
    "    # merge with step 1 body_part\n",
    "    df_step2_cancer_bpart = pd.concat([df_cancer_bpart,df_step2_cancer_bpart])\n",
    "\n",
    "    # drop duplicate values\n",
    "    df_step2_cancer_bpart = df_step2_cancer_bpart[[\"cancer_imaging_findings\",\"body_part\"]].drop_duplicates()\n",
    "    print(df_step2_cancer_bpart)   \n",
    "       \n",
    "    # logic 2-merge df_cancer_bpart,df_cancer_anat_bpart1,df_cancer_anat_bpart2 into df_step2_cancer_bpart\n",
    "    #df_step2_cancer_bpart = pd.concat([df_cancer_bpart,df_cancer_anat_bpart1[[\"cancer_imaging_findings\",\"body_part\"]],df_cancer_anat_bpart2[[\"cancer_imaging_findings\",\"body_part\"]] ]).drop_duplicates()\n",
    "\n",
    "    # remove \\n in body_part\n",
    "    df_step2_cancer_bpart[\"body_part\"] = df_step2_cancer_bpart[\"body_part\"].str.replace(\"\\n\",\"\",regex=False)\n",
    "    \n",
    "    # reset index\n",
    "    df_step2_cancer_bpart = df_step2_cancer_bpart.reset_index(drop=True)\n",
    "    print(\"\\n>>>step 2 sites after resolving anatomical to body_part: \\n\")\n",
    "    print(df_step2_cancer_bpart)\n",
    "\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 3 - process mapping file 02 map cancer+body_part > body_part\n",
    "    # 22-mar-2023: to handle no return value (eg cake + omental > no value)\n",
    "    ###############################################################################################  \n",
    "    print(\"\\n-------------------- step 3 --------------------\")      \n",
    "    print(\"***map cancer+body_part > output_body_part (mapping file 02)\\n\") \n",
    "\n",
    "    # use function to search against mapping 02\n",
    "    df_step2_cancer_bpart['02_body_part']=\"\"\n",
    "    for idx,row in df_step2_cancer_bpart.iterrows():\n",
    "        print(idx, row[\"cancer_imaging_findings\"],row[\"body_part\"])\n",
    "        output_body_part = get_mapping02_bpart(row[\"cancer_imaging_findings\"],row[\"body_part\"], df_mapping02)\n",
    "        #print(output_body_part)\n",
    "        df_step2_cancer_bpart.at[idx,'02_body_part'] = output_body_part\n",
    "\n",
    "    # fill null with \"\", and filter for rows with 02_body_part values \n",
    "    df_step2_cancer_bpart = df_step2_cancer_bpart.fillna(\"\")\n",
    "    df_step2_cancer_bpart = df_step2_cancer_bpart[df_step2_cancer_bpart[\"02_body_part\"]!=\"\"]   \n",
    "\n",
    "    # use set to get all 02_body_part \n",
    "    site = set(df_step2_cancer_bpart['02_body_part'])\n",
    "\n",
    "    print(\"\\n>>>step 3 sites after cancer+body_part mapping: \", site)\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 4 - process mapping file 03 map body_part to normalized_body_part\n",
    "    ############################################################################################### \n",
    "    print(\"\\n-------------------- step 4 --------------------\")      \n",
    "    print(\"***map body_part to normalized_body_part (mapping file 03)\\n\")\n",
    "    print(\"current site: \", site)\n",
    "    tempsite=set(site)\n",
    "    newsite=set()\n",
    "    if tempsite == set():\n",
    "        newsite=set()\n",
    "    else:\n",
    "        for s in tempsite:\n",
    "            try:\n",
    "                v = map_nbody_part_dict[s]\n",
    "                print(s, \"-> \",v)\n",
    "                newsite.add(v) \n",
    "            except:\n",
    "                # no map, append current site\n",
    "                print(s)\n",
    "                newsite.add(s)                 \n",
    "    print(\"\\n>>>step 4 sites after normalized body_part: \", newsite)\n",
    "\n",
    "    ###############################################################################################    \n",
    "    # step 5 - process mapping file 04 merge body part\n",
    "    ##############################################################################################      \n",
    "    print(\"\\n-------------------- step 5 --------------------\")      \n",
    "    print(\"***merge body part (mapping file 04)\\n\")\n",
    "    for index, row in mbody_part_df.iterrows():\n",
    "        #print(row)\n",
    "        combine_site=set([row.body_part1,row.body_part2])\n",
    "        if len(newsite.intersection(combine_site)) == 2:\n",
    "            print(\"to merge :\", combine_site)\n",
    "            newsite = newsite.difference(combine_site)\n",
    "            newsite.add(row.output_body_part)\n",
    "            print(\"\\n>>>step 5 sites after merge body_part: \", newsite)\n",
    "\n",
    "    ###############################################################################################                \n",
    "    # step 6- remove nodal if it still exists in output\n",
    "    # process mapping file 05 drop body part values\n",
    "    ###############################################################################################        \n",
    "    #newsite = newsite.difference({'nodal'})\n",
    "    print(\"\\n-------------------- step 6 --------------------\")         \n",
    "    print(\"***drop body part (mapping file 05)\\n\")\n",
    "    sites_to_drop = newsite.intersection(map_to_drop)\n",
    "    if len(sites_to_drop) > 0:\n",
    "        print(\">>>dropping site from mapping 05: \",sites_to_drop )\n",
    "    newsite = newsite.difference(map_to_drop)\n",
    "    print(\"\\n>>>step 6 sites after merge body_part: \", newsite, \"\\n\")\n",
    "    \n",
    "    print(\"\\n>>>final sites to output :\", newsite, \"\\n\")       \n",
    "\n",
    "    mets_df.at[i, 'pred_site_of_mets'] = set(newsite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = input_filename.replace(\".csv\",\"_pred_site_of_mets.csv\")\n",
    "print(\"saving predictions to: \", output_filename)\n",
    "mets_df.to_csv(os.path.join(input_folder,output_filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b470e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check visualisation for specific case\n",
    "i=0\n",
    "\n",
    "text = input_df['conclusion'].loc[i].lower()\n",
    "annotations = light_model.fullAnnotate(text)\n",
    "\n",
    "rel_df = get_relations_df(annotations,\"mets_relations\")\n",
    "rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fbfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.display(annotations[0], 'mets_relations', show_relations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48225e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
