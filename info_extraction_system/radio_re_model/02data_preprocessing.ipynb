{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7de342",
   "metadata": {},
   "source": [
    "## Purpose: To preprocess/clean all csv files converted from xmi for RE Sites of Metastases model training\n",
    "\n",
    "RE model training (required entity pairs):\n",
    "- cancer imaging findings - body part\n",
    "- cancer imaging findings - anatomical descriptor\n",
    "- anatomical descriptor - body part\n",
    "- direction - body part\n",
    "- direction - anatomical descriptor\n",
    "\n",
    "refer: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/10.Clinical_Relation_Extraction.ipynb#scrollTo=6doZTPX_xnEm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ba59bb7",
   "metadata": {},
   "source": [
    "#Notebook directory structure\n",
    "    > dataset : contains input files\n",
    "        > 01xmi\n",
    "        > 02csv\n",
    "    > saved_models : contains output re models\n",
    "    > re_graph     : tensorflow graph file\n",
    "    > re_result    : model performance metrics\n",
    "    > re_logs      : training logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3589671",
   "metadata": {},
   "source": [
    "### Note: Before running this notebook, please configure the following paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81793fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure folder path\n",
    "file_path = r'dataset\\02csv'\n",
    "input_train_folder = 'csv_train'\n",
    "input_test_folder = 'csv_test'\n",
    "print(file_path+'\\\\'+input_train_folder)\n",
    "print(file_path+'\\\\'+input_test_folder)\n",
    "\n",
    "# dataset file name\n",
    "dataset_name=\"train4522"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1203f2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, datetime, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab2a51",
   "metadata": {},
   "source": [
    "## Import and merge all csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check for valid entity pair\n",
    "def check_pair(df, p):\n",
    "    temp = df[df['pairs'] == p]['check'].values\n",
    "    #print(\"***\",temp)\n",
    "    if len(temp) > 0 :\n",
    "        #print(df[df['pairs'] == p]['relation'][0])\n",
    "        return temp[0]\n",
    "    else:\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all train/test csv files into dataframe\n",
    "all_train_files = glob.glob(os.path.join(file_path, input_train_folder, \"*.csv\"))\n",
    "all_test_files = glob.glob(os.path.join(file_path, input_test_folder, \"*.csv\"))\n",
    "\n",
    "df_train_csv = pd.concat((pd.read_csv(f) for f in all_train_files))\n",
    "df_train_csv = df_train_csv.drop('Unnamed: 0',axis=1)\n",
    "df_train_csv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_test_csv = pd.concat((pd.read_csv(f) for f in all_test_files))\n",
    "df_test_csv = df_test_csv.drop('Unnamed: 0',axis=1)\n",
    "df_test_csv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# remove \\r\\n in sentence column\n",
    "df_train_csv['sentence'].replace('\\n', '', regex=True, inplace=True)\n",
    "df_train_csv['sentence'].replace('\\r', '', regex=True, inplace=True)\n",
    "df_test_csv['sentence'].replace('\\n', '', regex=True, inplace=True)\n",
    "df_test_csv['sentence'].replace('\\r', '', regex=True, inplace=True)\n",
    "\n",
    "# add column for entity pairs\n",
    "df_train_csv['pairs'] = (df_train_csv['entity1']+'-'+df_train_csv['entity2']).str.lower()\n",
    "df_test_csv['pairs'] = (df_test_csv['entity1']+'-'+df_test_csv['entity2']).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0773a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df_train_csv and df_test_csv to df_csv for subsequent processing\n",
    "# add column dataset to indicate train/test\n",
    "\n",
    "df_train_csv[\"dataset\"]=\"train\"\n",
    "df_test_csv[\"dataset\"]=\"test\"\n",
    "df_csv = pd.concat([df_train_csv,df_test_csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ec6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.groupby(\"dataset\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ced24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2904a",
   "metadata": {},
   "source": [
    "## check for correct entity pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743074d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: relation_group.csv contains all annotated pairs extracted from train csv\n",
    "# based on data checking, there are pairs annotated wrongly or in the wrong direction. These will be excluded from training.\n",
    "# for training of sites of mets, we are only looking at specific pairs, this is indicated in the column sites_of_mets_group\n",
    "# for better accuracy, we may also combine multiple pairs into same relation, \n",
    "# eg direction-body part and direction-anatomcal descriptor are combined as direction-of\n",
    "# the relation name will be used in RE visualisation, so cannot be too long (max 30 chars)\n",
    "\n",
    "df_relation = pd.read_csv(os.path.join(file_path,\"relation_group.csv\"))\n",
    "df_relation.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['check'] = df_csv.apply(lambda row: check_pair(df_relation,row['pairs']), axis=1)\n",
    "\n",
    "df_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file for checking\n",
    "df_csv.to_csv(os.path.join(file_path,\"radio_re_\"+dataset_name+\"_allrelations_check.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f75a66",
   "metadata": {},
   "source": [
    "## Data Cleaning (drop wrong/reverse/unknown pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are unknown entity pairs, check with annotation team and update the relation_group.csv accordingly\n",
    "# this can happen if they annotate new reports with new entity pairs which is not found in current file\n",
    "# currently, there is no unknown cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27-Jul-2022 check for pairs with unknown/wrong/reverse relation tag, drop them\n",
    "df_csv[df_csv['check'].isin(['unknown','wrong','reverse'])].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff36e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_clean = df_csv.copy()\n",
    "\n",
    "print(\"count before drop:\", df_csv_clean['check'].count())\n",
    "df_csv_clean.drop(df_csv_clean[df_csv_clean['check'].isin(['unknown','wrong','reverse'])].index,inplace=True)\n",
    "print(\"count after drop:\", df_csv_clean['check'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b93d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_clean.to_csv(os.path.join(file_path,\"radio_re_\"+dataset_name+\"_allrelations_clean.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844983e",
   "metadata": {},
   "source": [
    "## Extract required pairs for RE Sites of Mets Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mets = df_relation[pd.notnull(df_relation['sites_of_mets_group'])][['pairs','sites_of_mets_group']]\n",
    "df_csv_clean = pd.merge(df_csv_clean, df_mets, on='pairs', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8481ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pair-pair as relation name, not re-grouping\n",
    "df_csv_clean['relation'] = df_csv_clean['sites_of_mets_group']\n",
    "df_csv_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4209ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null relation, entity1, entity2\n",
    "df_csv_clean = df_csv_clean.dropna(axis=0, subset=['relation'])\n",
    "df_csv_clean = df_csv_clean.dropna(axis=0, subset=['entity1'])\n",
    "df_csv_clean = df_csv_clean.dropna(axis=0, subset=['entity2'])\n",
    "df_csv_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_clean = df_csv_clean.reset_index()\n",
    "\n",
    "# extract required columns\n",
    "columns = [\"relation\",\"pairs\",\"entity1\",\"chunk1\",\"entity2\",\"chunk2\",\"entity1_begin\",\"entity1_end\",\"entity2_begin\",\"entity2_end\",\"doc_text\",\"doc_title\",\"dataset\"]\n",
    "df_csv_clean = df_csv_clean[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4bab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667066a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_clean.groupby('dataset').count()['relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d943809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_clean.groupby(['dataset','relation']).count()['pairs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c11041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv for training pipeline\n",
    "df_csv_clean.to_csv(os.path.join(file_path,\"radio_re_\"+dataset_name+\"_sitesofmets_relations_clean.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
