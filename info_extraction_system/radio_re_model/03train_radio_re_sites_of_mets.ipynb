{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7de342",
   "metadata": {},
   "source": [
    "## Purpose: To train/evaluate a custom relation extraction model/prediction pipeline for Sites of Metastases \n",
    "\n",
    "RE model training (required entity pairs):\n",
    "- cancer imaging findings - body part\n",
    "- cancer imaging findings - anatomical descriptor\n",
    "- anatomical descriptor - body part\n",
    "- direction - body part [this is currently not used in sites of mets prediction]\n",
    "- direction - anatomical descriptor [this is currently not used in sites of mets prediction]\n",
    "\n",
    "Single pipeline for Sites of Mets prediction:\n",
    "- custom NER\n",
    "- custom Assertion\n",
    "- custom RE\n",
    "- post processing for term normalization (using mapping files)\n",
    "\n",
    "refer: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/10.Clinical_Relation_Extraction.ipynb#scrollTo=6doZTPX_xnEm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ba59bb7",
   "metadata": {},
   "source": [
    "#Notebook directory structure\n",
    "    > dataset : contains input files\n",
    "        > 01xmi\n",
    "        > 02csv\n",
    "    > saved_models : contains output re models\n",
    "    > re_graph     : tensorflow graph file\n",
    "    > re_output    : model output file\n",
    "    > re_result    : model performance metrics\n",
    "    > re_logs      : training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42847ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to run to create the subfolders, for the first time\n",
    "#!mkdir re_graphs saved_models re_output re_result re_logs inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3589671",
   "metadata": {},
   "source": [
    "### Note: Before running this notebook, please configure the following paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using sparknlp clinical embedding word model\n",
    "# specify your folder containing the downloaded clinical embedding word model file, or you can use .pretrained during training instead to load it online\n",
    "embeddings_clinical_local_path = r\"path\\to\\sparknlp_pretrained\\embeddings_clinical_en_2.4.0_2.4_1580237286004\"\n",
    "model_type = \"clinical_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6808f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your sparknlp online license key-need internet connection\n",
    "# we are using v3.4.2\n",
    "sparknlp_licence_key = r\"..\\sparknlp_licence_key\\yourkey.json\"\n",
    "\n",
    "# specify your sparknlp offline license key-airgap env\n",
    "# we are using v3.4.2\n",
    "sparknlp_airgap_licence_key = r\"..\\sparknlp_licence_key\\yourairgapkey.json\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d28bcf0",
   "metadata": {},
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1203f2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, sparknlp, sparknlp_jsl, datetime, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.training import CoNLL\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp_jsl.training import tf_graph\n",
    "from sparknlp_display import AssertionVisualizer, NerVisualizer,RelationExtractionVisualizer \n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118e80f",
   "metadata": {},
   "source": [
    "## Start Spark Session (offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766fff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline-Load airgap license key\n",
    "with open(sparknlp_airgap_licence_key) as f:\n",
    "    airgap_license_keys = json.load(f)\n",
    "    \n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(airgap_license_keys)\n",
    "os.environ.update(airgap_license_keys)\n",
    "\n",
    "# check variable\n",
    "!echo $SECRET\n",
    "!echo $JSL_VERSION\n",
    "!echo $PUBLIC_VERSION\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])\n",
    "\n",
    "# use this 20-sep-2022\n",
    "# Start Spark Session with Custom Params (OFFLINE)\n",
    "# https://spark.apache.org/docs/latest/configuration.html#memory-management\n",
    "# Important! memory setting need to be adjusted for different work load \n",
    "\n",
    "def start(SECRET):\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed radio mets jupyter\") \\\n",
    "        .master(\"local[48]\") \\\n",
    "        .config(\"spark.driver.memory\", \"90G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.driver.maxResultSize\",\"8000M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\") \\\n",
    "        .config(\"spark.jars\", f\"d:\\content\\spark-nlp-jsl-{JSL_VERSION}.jar, d:\\airgap\\spark-nlp_2.12-3.4.2.jar\" )\n",
    "\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = start(SECRET) \n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935759f0",
   "metadata": {},
   "source": [
    "## Start Spark Session (online)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a663c",
   "metadata": {},
   "source": [
    "Note: Requires Spark NLP and Spark NLP for Healthcare (licensed version) packages to be installed"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2bab3ae",
   "metadata": {},
   "source": [
    "# NEW, use this code, no need to define env variables\n",
    "# Load license key\n",
    "with open(sparknlp_licence_key) as f:\n",
    "    license_keys = json.load(f)\n",
    "    \n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "\n",
    "# Adding license key-value pairs to environment variables, use this (17-Aug-2022)\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "\n",
    "# check variable\n",
    "!echo $SECRET\n",
    "!echo $JSL_VERSION\n",
    "!echo $PUBLIC_VERSION\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b201e5ec",
   "metadata": {},
   "source": [
    "# https://spark.apache.org/docs/latest/configuration.html#memory-management\n",
    "# Important! memory setting need to be adjusted for different work load \n",
    "\n",
    "# Start Spark Session\n",
    "params = {\"spark.master\":\"local[32]\",\n",
    "          \"spark.driver.memory\":\"80G\", \n",
    "          \"spark.kryoserializer.buffer.max\":\"2000M\", \n",
    "          \"spark.driver.maxResultSize\":\"8000M\",\n",
    "          \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "          \"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\",\n",
    "          \"spark.jars\": \"https://pypi.johnsnowlabs.com/\"+SECRET+\"/spark-nlp-jsl-\"+JSL_VERSION+\".jar\",\n",
    "          \"spark.local.dir\": r\"c:\\users\\guathwa\\spark-temp\"\n",
    "           } \n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab2a51",
   "metadata": {},
   "source": [
    "## Import train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93887629",
   "metadata": {},
   "outputs": [],
   "source": [
    "## configure folder path\n",
    "data_folder = \"dataset\"\n",
    "train_folder = data_folder+\"\\\\02csv\"\n",
    "dataset_name = \"train4522\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d73dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in saved train.csv\n",
    "df_csv_clean = pd.read_csv(os.path.join(train_folder,\"radio_re_\"+dataset_name+\"_sitesofmets_relations_clean.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f955d0",
   "metadata": {},
   "source": [
    "## create pyspark schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e764ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "#Create User defined Custom Schema using StructType\n",
    "df1Schema = StructType([StructField(\"relation\", StringType(), True)\\\n",
    "                        ,StructField(\"pairs\", StringType(), True)\\\n",
    "                        ,StructField(\"entity1\", StringType(), True)\\\n",
    "                        ,StructField(\"chunk1\", StringType(), True)\\\n",
    "                        ,StructField(\"entity2\", StringType(), True)\\\n",
    "                        ,StructField(\"chunk2\", StringType(), True)\\\n",
    "                        ,StructField(\"entity1_begin\", IntegerType(), True)\\\n",
    "                        ,StructField(\"entity1_end\", IntegerType(), True)\\\n",
    "                        ,StructField(\"entity2_begin\", IntegerType(), True)\\\n",
    "                        ,StructField(\"entity2_end\", IntegerType(), True)\\\n",
    "                        ,StructField(\"doc_text\", StringType(), True)\\\n",
    "                        ,StructField(\"doc_title\", StringType(), True)\\\n",
    "                        ,StructField(\"dataset\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32dd098",
   "metadata": {},
   "source": [
    "## create spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.createDataFrame(df_csv_clean, schema=df1Schema) \n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a423e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by dataset,relation\n",
    "data.groupby(['dataset','relation']).count().sort(['dataset','relation']).show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8b356",
   "metadata": {},
   "source": [
    "## Create a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126945bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q tensorflow-addons"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f01e17c3",
   "metadata": {},
   "source": [
    "# if you need to customize the DL arcitecture (more layers, more features etc.)\n",
    "\n",
    "from sparknlp_jsl.training import tf_graph\n",
    "\n",
    "tf_graph.build(\"relation_extraction\", build_params={\"input_dim\": 1200, \"output_dim\": 57, 'batch_norm':1, \"hidden_layers\": [300, 200], \"hidden_act\": \"relu\", 'hidden_act_l2':1}, model_location=\"./re_graph\", model_filename=\"rel_in1200_out57.pb\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c83229ca",
   "metadata": {},
   "source": [
    "tf_graph.print_model_params(\"relation_extraction\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07a07c34",
   "metadata": {},
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Output Dataframe to store model performance metrics\n",
    "output_df = pd.DataFrame(columns = ['re_model','ner_model','rels_set','trainset_count', 'testset_count','epoch', 'learning_rate', 'batch_size','start_time', 'end_time', 'duration', 'overall_accuracy','class_accuracy', 'classification_report','confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21294390",
   "metadata": {},
   "source": [
    "## Data Preparation (RE_SITES_OF_METS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8620ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "\n",
    "#Annotation structure\n",
    "annotationType = T.StructType([\n",
    "            T.StructField('annotatorType', T.StringType(), False),\n",
    "            T.StructField('begin', T.IntegerType(), False),\n",
    "            T.StructField('end', T.IntegerType(), False),\n",
    "            T.StructField('result', T.StringType(), False),\n",
    "            T.StructField('metadata', T.MapType(T.StringType(), T.StringType()), False),\n",
    "            T.StructField('embeddings', T.ArrayType(T.FloatType()), False)\n",
    "        ])\n",
    "\n",
    "#UDF function to convert train data to names entitities\n",
    "\n",
    "@F.udf(T.ArrayType(annotationType))\n",
    "def createTrainAnnotations(begin1, end1, begin2, end2, chunk1, chunk2, label1, label2):\n",
    "    \n",
    "    entity1 = sparknlp.annotation.Annotation(\"chunk\", begin1, end1, chunk1, {'entity': label1.lower(), 'sentence': '0'}, [])\n",
    "    entity2 = sparknlp.annotation.Annotation(\"chunk\", begin2, end2, chunk2, {'entity': label2.lower(), 'sentence': '0'}, [])    \n",
    "        \n",
    "    entity1.annotatorType = \"chunk\"\n",
    "    entity2.annotatorType = \"chunk\"\n",
    "\n",
    "    return [entity1, entity2]    \n",
    "\n",
    "train_pair = ['cancer_imaging_findings-body_part', \\\n",
    "            'cancer_imaging_findings-anatomical_descriptor', \\\n",
    "            'anatomical_descriptor-body_part',\\\n",
    "            'direction-body_part',\\\n",
    "            'direction-anatomical_descriptor']\n",
    "\n",
    "# start of data preparation\n",
    "valid_rel_query = \"(\" + \" OR \".join([\"pairs = '{}'\".format(p) for p in train_pair]) + \")\"\n",
    "print(valid_rel_query)\n",
    "\n",
    "data2 = data\\\n",
    "  .withColumn(\"entity1_begin\", F.expr(\"cast(entity1_begin AS Int)\"))\\\n",
    "  .withColumn(\"entity1_end\", F.expr(\"cast(entity1_end AS Int)\"))\\\n",
    "  .withColumn(\"entity2_begin\", F.expr(\"cast(entity2_begin AS Int)\"))\\\n",
    "  .withColumn(\"entity2_end\", F.expr(\"cast(entity2_end AS Int)\"))\\\n",
    "  .where(\"entity1_begin IS NOT NULL\")\\\n",
    "  .where(\"entity1_end IS NOT NULL\")\\\n",
    "  .where(\"entity2_begin IS NOT NULL\")\\\n",
    "  .where(\"entity2_end IS NOT NULL\")\\\n",
    "  .where(valid_rel_query)\\\n",
    "  .withColumn(\n",
    "      \"train_ner_chunks\", \n",
    "      createTrainAnnotations(\n",
    "          \"entity1_begin\", \"entity1_end\", \"entity2_begin\", \"entity2_end\", \"chunk1\", \"chunk2\", \"entity1\", \"entity2\"\n",
    "      ).alias(\"train_ner_chunks\", metadata={'annotatorType': \"chunk\"}))\n",
    "\n",
    "train_data = data2.where(\"dataset='train'\")\n",
    "test_data = data2.where(\"dataset='test'\")\n",
    "\n",
    "#===================================================\n",
    "print(\"total row count:\",data2.count())\n",
    "\n",
    "trainset_count = train_data.groupby('pairs').count().collect()\n",
    "testset_count = test_data.groupby('pairs').count().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby('relation').count().sort('relation').show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.groupby('relation').count().sort('relation').show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52022c67",
   "metadata": {},
   "source": [
    "## ------------------- START OF TRAINING  --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b296da6",
   "metadata": {},
   "source": [
    "## Training Pipeline (RE_SITES_OF_METS)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "928548d6",
   "metadata": {},
   "source": [
    "# often encountered spark error during training, need to adjust spark session memory setting\n",
    "Py4JJavaError: An error occurred while calling o277.fit.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 39.0 failed 1 times, most recent failure: Lost task 1.0 in stage 39.0 (TID 2478) (DESKTOP-PISHPGG.mshome.net executor driver): java.net.SocketException: Connection reset\n",
    "\n",
    "# check spark jobs\n",
    "http://localhost:4040/stages/\n",
    "http://localhost:4040/executors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a716559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the name of NER model\n",
    "radio_ner_model = \"clinical_embeddings_5_8_0.001_u0.4o1_train4522\"\n",
    "\n",
    "# specify the name of assertion model\n",
    "radio_assertion_model = \"radio_assertion_model_10_16_0.001_2022_11_18_15_39_34\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb032e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sentencer, change setInputCol from sentences to document as our training chunk start/end position is with respect to each document\n",
    "\n",
    "# training hyperparameters\n",
    "#epoch = 70\n",
    "epoch = 50\n",
    "#batch_size = 8\n",
    "batch_size = 16\n",
    "#learning_rate = 0.001\n",
    "learning_rate = 0.005\n",
    "\n",
    "#======================================\n",
    "# start training\n",
    "#======================================\n",
    "start = time.ctime()\n",
    "start2 = time.time()\n",
    "print('start time for training: ', start)\n",
    "print('...setup training pipeline')\n",
    "\n",
    "# this is for document level RE\n",
    "documenter = DocumentAssembler()\\\n",
    "    .setInputCol(\"doc_text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"tokens\")\\\n",
    "\n",
    "words_embedder = WordEmbeddingsModel()\\\n",
    "    .load(embeddings_clinical_local_path)\\\n",
    "    .setInputCols([\"document\", \"tokens\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "#use .pretrained(\"pos_clinical\", \"en\", \"clinical/models\") for sparknlp online session\n",
    "#use .load() for sparknlp airgap session\n",
    "pos_tagger = PerceptronModel()\\\n",
    "    .load(\"./pretrained/pos_clinical_en_3.0.0_3.0_1617052315327\") \\\n",
    "    .setInputCols([\"document\", \"tokens\"])\\\n",
    "    .setOutputCol(\"pos_tags\")\n",
    "\n",
    "#use .pretrained(\"dependency_conllu\", \"en\") for sparknlp online session\n",
    "#use .load() for sparknlp airgap session\n",
    "dependency_parser = DependencyParserModel()\\\n",
    "    .load(\"./pretrained/dependency_conllu_en_3.0.0_3.0_1656858083101\")\\\n",
    "    .setInputCols([\"document\", \"pos_tags\", \"tokens\"])\\\n",
    "    .setOutputCol(\"dependencies\")\n",
    "\n",
    "# set training params and upload model graph (see ../Healthcare/8.Generic_Classifier.ipynb)\n",
    "reApproach = RelationExtractionApproach()\\\n",
    "    .setInputCols([\"embeddings\", \"pos_tags\", \"train_ner_chunks\", \"dependencies\"])\\\n",
    "    .setOutputCol(\"relations\")\\\n",
    "    .setLabelColumn(\"relation\")\\\n",
    "    .setEpochsNumber(epoch)\\\n",
    "    .setBatchSize(batch_size)\\\n",
    "    .setDropout(0.2)\\\n",
    "    .setLearningRate(learning_rate)\\\n",
    "    .setModelFile(\"./re_graph/rel_in1200_out57.pb\")\\\n",
    "    .setFixImbalance(True)\\\n",
    "    .setValidationSplit(0.10)\\\n",
    "    .setFromEntity(\"entity1_begin\", \"entity1_end\", \"entity1\")\\\n",
    "    .setToEntity(\"entity2_begin\", \"entity2_end\", \"entity2\")\\\n",
    "    .setOutputLogsPath('./re_logs')\n",
    "\n",
    "finisher = Finisher()\\\n",
    "    .setInputCols([\"relations\"])\\\n",
    "    .setOutputCols([\"relations_out\"])\\\n",
    "    .setCleanAnnotations(False)\\\n",
    "    .setValueSplitSymbol(\",\")\\\n",
    "    .setAnnotationSplitSymbol(\",\")\\\n",
    "    .setOutputAsArray(False)\n",
    "\n",
    "re_mets_train_pipeline = Pipeline(stages=[\n",
    "    documenter, \n",
    "    tokenizer, \n",
    "    words_embedder, \n",
    "    pos_tagger, \n",
    "    dependency_parser, \n",
    "    reApproach,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "print('...train model')\n",
    "%time re_mets_model = re_mets_train_pipeline.fit(train_data)\n",
    "print('...training completed')\n",
    "done = time.ctime()\n",
    "done2 = time.time()\n",
    "duration = done2-start2\n",
    "print('end time for training: ', done)\n",
    "#======================================\n",
    "# end training\n",
    "#======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88426061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "re_mets_model_name = \"re_sites_of_mets_\"+str(epoch)+\"_\"+str(batch_size)+\"_\"+str(learning_rate)+\"_\"+str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")+\"_\"+dataset_name)\n",
    "print('...save models to folder: ./saved_models/'+re_mets_model_name)\n",
    "re_mets_model.stages[-2].write().overwrite().save('./saved_models/'+re_mets_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================\n",
    "# model evaluation\n",
    "#======================================\n",
    "print('...evaluate model')\n",
    "pred_result = re_mets_model.transform(test_data)\n",
    "print('...evaluation completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad31aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result.select('relation','entity1','chunk1','entity2','chunk2','relations_out').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so save prediction in csv using parquet write, takes about 5mins, depending on filesize\n",
    "filename = './re_output/'+re_mets_model_name+'_prediction'\n",
    "print(filename)\n",
    "pred_result.select('relation','entity1','chunk1','entity2','chunk2','relations_out').coalesce(1).write.options(header=True).mode('overwrite').csv(filename)\n",
    "\n",
    "# generated parquet csv filename is in this format : part-00000-c4d1d33c-1254-4716-9866-44a79e73be35-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8153442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output filename in the above folder\n",
    "# eg part-00000-199c0e31-85c9-4f4c-a894-d6ca8cdaaf38-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in parquet csv to get y_true, y_pred\n",
    "pred_df = pd.read_csv('./re_output/'+re_mets_model_name+'_prediction'+'//'+'part-00000-199c0e31-85c9-4f4c-a894-d6ca8cdaaf38-c000.csv')\n",
    "pred_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa62c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model performance metrics\n",
    "\n",
    "y_true = pred_df[\"relation\"].values\n",
    "y_pred = pred_df[\"relations_out\"].values\n",
    "accuracy = accuracy_score(y_true,y_pred)\n",
    "print(\"accuracy: \", accuracy)\n",
    "\n",
    "report = classification_report(y_true,y_pred, digits=4, labels=np.unique(y_true))\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_true,y_pred)\n",
    "print(cm)\n",
    "\n",
    "# get per class accuracy\n",
    "# https://stackoverflow.com/questions/39770376/scikit-learn-get-accuracy-scores-for-each-class\n",
    "classes=np.unique(y_true)\n",
    "\n",
    "# We will store the results in a dictionary for easy access later\n",
    "per_class_accuracies = {}\n",
    "\n",
    "# Calculate the accuracy for each one of our classes\n",
    "for idx, cls in enumerate(classes):\n",
    "    # True negatives are all the samples that are not our current GT class (not the current row) \n",
    "    # and were not predicted as the current class (not the current column)\n",
    "    true_negatives = np.sum(np.delete(np.delete(cm, idx, axis=0), idx, axis=1))\n",
    "    \n",
    "    # True positives are all the samples of our current GT class that were predicted as such\n",
    "    true_positives = cm[idx, idx]\n",
    "    \n",
    "    # The accuracy for the current class is ratio between correct predictions to all predictions      \n",
    "    # 03-jul-2023: dont consider TN, use TP/(TP+FP+FN), same formulae for whole manuscript\n",
    "    per_class_accuracies[cls] = (true_positives) / (np.sum(cm)-true_negatives) \n",
    "\n",
    "    \n",
    "# Combine class accuracies to classification report\n",
    "report_dict = classification_report(y_true,y_pred, digits=4, labels=np.unique(y_true), output_dict=True)\n",
    "classification_report_df = pd.DataFrame(report_dict).transpose()\n",
    "per_class_accuracies_df = pd.DataFrame.from_dict(per_class_accuracies, orient='index', columns=['class_accuracy']) \n",
    "combine_report_df = pd.concat([per_class_accuracies_df,classification_report_df], axis=1)\n",
    "\n",
    "# save performance to csv\n",
    "# model,rels_set,trainset_count,epoch,learning_rate,batch_size,start_time,end_time,duration,accuracy,classification_report,confusion_matrix\n",
    "to_append = [re_mets_model_name,radio_ner_model,train_pair,trainset_count,testset_count,epoch,learning_rate,batch_size,start,done,duration,accuracy,per_class_accuracies_df,report,cm]\n",
    "df_length = len(output_df)\n",
    "\n",
    "output_df.loc[df_length] = to_append\n",
    "filename_prefix = \"./re_result/radio_re_sites_of_mets\" + \"_\" + str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
    "filename = \"%s.csv\" % filename_prefix\n",
    "output_df.to_csv(filename, header=True)\n",
    "print(50*'-')\n",
    "print(\"<<<Model Performance saved!>>>\")\n",
    "print(50*'-')\n",
    "print(50*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b157a",
   "metadata": {},
   "source": [
    "## ------------------- END OF TRAINING  --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257defd",
   "metadata": {},
   "source": [
    "## ------------------- MODEL INFERENCE --------------------\n",
    "same code as 04predict_radio_re_sites_of_mets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a18bd4",
   "metadata": {},
   "source": [
    "### Test single Prediction Pipeline NER + Assertion Detection + RE_SITES_OF_METS)\n",
    "copy the radio NER model and radio cancer assertion model to be used for this pipeline in radio_re_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e59e6d",
   "metadata": {},
   "source": [
    "### Load trained model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_relations_df (results, col='relations'):\n",
    "  rel_pairs=[]\n",
    "  for rel in results[0][col]:\n",
    "      rel_pairs.append((\n",
    "          rel.result, \n",
    "          rel.metadata['entity1'], \n",
    "          rel.metadata['entity1_begin'],\n",
    "          rel.metadata['entity1_end'],\n",
    "          rel.metadata['chunk1'], \n",
    "          rel.metadata['entity2'],\n",
    "          rel.metadata['entity2_begin'],\n",
    "          rel.metadata['entity2_end'],\n",
    "          rel.metadata['chunk2'], \n",
    "          rel.metadata['confidence']\n",
    "      ))\n",
    "\n",
    "  rel_df = pd.DataFrame(rel_pairs, columns=['relation','entity1','entity1_begin','entity1_end','chunk1','entity2','entity2_begin','entity2_end','chunk2', 'confidence'])\n",
    "\n",
    "  return rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e53201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common block\n",
    "# 11-Aug-2022 add chunk filterer to filter entities of interest\n",
    "documenter = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentencer = SentenceDetector()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentences\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"sentences\"])\\\n",
    "    .setOutputCol(\"tokens\")\\\n",
    "\n",
    "words_embedder = WordEmbeddingsModel()\\\n",
    "    .load(embeddings_clinical_local_path)\\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "pos_tagger = PerceptronModel()\\\n",
    "    .load(\"./pretrained/pos_clinical_en_3.0.0_3.0_1617052315327\") \\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"pos_tags\")\n",
    "\n",
    "dependency_parser = DependencyParserModel()\\\n",
    "    .load(\"./pretrained/dependency_conllu_en_3.0.0_3.0_1656858083101\")\\\n",
    "    .setInputCols([\"sentences\", \"pos_tags\", \"tokens\"])\\\n",
    "    .setOutputCol(\"dependencies\")\n",
    "\n",
    "# to detect all radio ner\n",
    "radio_ner = MedicalNerModel.load(radio_ner_model)\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"ner_tags\")\n",
    "\n",
    "# to get all radio ner chunks\n",
    "radio_ner_converter = NerConverter()\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "    .setOutputCol(\"ner_chunks\")\n",
    "\n",
    "# to filter for required ners\n",
    "non_cancer_ner_converter = NerConverter()\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "    .setOutputCol(\"non_cancer_ner_chunks\")\\\n",
    "    .setWhiteList([\"body_part\",\"anatomical_descriptor\",\"direction\",\"probability_high\",\"probability_medium\",\"probability_uncertain\",\"probability_low\"])\n",
    "\n",
    "## to filter for cancer_imaging_findings ner chunks, and send for assertion detection\n",
    "cancer_ner_converter = NerConverter()\\\n",
    "        .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "        .setOutputCol(\"cancer_ner_chunks\")\\\n",
    "        .setWhiteList([\"cancer_imaging_findings\"])\n",
    "\n",
    "## assertion detection for cancer_imaging_findings\n",
    "cancer_ner_assertion = AssertionDLModel.load(radio_assertion_model) \\\n",
    "    .setInputCols([\"sentences\", \"cancer_ner_chunks\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"cancer_assertion\")\n",
    "\n",
    "## add filterer to filter for probability_high/probability_medium\n",
    "cancer_ner_assertion_filterer = AssertionFilterer()\\\n",
    "    .setInputCols(\"sentences\",\"cancer_ner_chunks\",\"cancer_assertion\")\\\n",
    "    .setOutputCol(\"cancer_assertion_filtered\")\\\n",
    "    .setWhiteList([\"probability_high\",\"probability_medium\"])\n",
    "\n",
    "# merge ner_chunks by prioritizing the overlapping indices (chunks with longer lengths and highest information will be kept from each ner model)\n",
    "chunk_merger = ChunkMergeApproach()\\\n",
    "    .setInputCols('non_cancer_ner_chunks', \"cancer_assertion_filtered\")\\\n",
    "    .setOutputCol('merged_ner_chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf777c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to set the entity pairs you need for this prediction pipeline\n",
    "pairs_mets = ['cancer_imaging_findings-body_part', \\\n",
    "            'cancer_imaging_findings-anatomical_descriptor', \\\n",
    "            'anatomical_descriptor-body_part', \\\n",
    "            'direction-body_part',\n",
    "            'direction-anatomical_descriptor'\n",
    "            ]             \n",
    "\n",
    "mets_rel_pairs = pairs_mets\n",
    "\n",
    "##============================================================================\n",
    "loaded_re_mets_model = RelationExtractionModel()\\\n",
    "    .load('./saved_models/'+re_mets_model_name)\\\n",
    "    .setInputCols([\"embeddings\", \"pos_tags\", \"merged_ner_chunks\", \"dependencies\"])\\\n",
    "    .setOutputCol(\"mets_relations\")\\\n",
    "    .setRelationPairs(mets_rel_pairs)\\\n",
    "    .setMaxSyntacticDistance(4)\\\n",
    "    .setPredictionThreshold(0.8)\n",
    "\n",
    "re_mets_pipeline = Pipeline(stages=[\n",
    "    documenter,\n",
    "    sentencer,\n",
    "    tokenizer, \n",
    "    words_embedder, \n",
    "    pos_tagger, \n",
    "    dependency_parser,\n",
    "    radio_ner,\n",
    "    radio_ner_converter,\n",
    "    non_cancer_ner_converter,\n",
    "    cancer_ner_converter,\n",
    "    cancer_ner_assertion,\n",
    "    cancer_ner_assertion_filterer,\n",
    "    chunk_merger,\n",
    "    loaded_re_mets_model\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "re_mets_pipeline_model = re_mets_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# site of mets\n",
    "mtext1 = \"\"\"\n",
    "your sample text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = mtext1\n",
    "sample_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "sample_data.show()\n",
    "sample_data.dtypes\n",
    "\n",
    "model = re_mets_pipeline_model\n",
    "#model = re_mets_merge_pipeline_model\n",
    "preds = model.transform(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cancer assertion status\n",
    "preds.select(F.explode(F.arrays_zip(preds.cancer_ner_chunks.result, \n",
    "                                     preds.cancer_ner_chunks.metadata, \n",
    "                                     preds.cancer_assertion.result)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunks\"),\n",
    "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
    "              F.expr(\"cols['1']['sentences']\").alias(\"sent_id\"),\n",
    "              F.expr(\"cols['2']\").alias(\"assertion\")).show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba896e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cancer assertion filterer\n",
    "preds.select(F.explode(F.arrays_zip(preds.cancer_ner_chunks.result, \n",
    "                                     preds.cancer_ner_chunks.metadata, \n",
    "                                     preds.cancer_assertion_filtered.result)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunks\"),\n",
    "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
    "              F.expr(\"cols['1']['sentences']\").alias(\"sent_id\"),\n",
    "              F.expr(\"cols['2']\").alias(\"filtered_assertion\")).show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fea8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check merged_ner_chunks\n",
    "preds.select(F.explode(F.arrays_zip(preds.merged_ner_chunks.result, \n",
    "                                     preds.merged_ner_chunks.metadata)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunks\"),\n",
    "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
    "              F.expr(\"cols['1']['sentences']\").alias(\"sent_id\")).show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33deebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = preds.select(F.explode(F.arrays_zip('mets_relations.result', 'mets_relations.metadata')).alias(\"cols\")) \\\n",
    ".select(F.expr(\"cols['0']\").alias(\"relations\"),\n",
    "        F.expr(\"cols['1']['sentence']\").alias(\"sentence_id\"),\n",
    "        F.expr(\"cols['1']['entity1']\").alias(\"entity1\"),\n",
    "#        F.expr(\"cols['1']['entity1_begin']\").alias(\"entity1_begin\"),\n",
    "#        F.expr(\"cols['1']['entity1_end']\").alias(\"entity1_end\"),\n",
    "        F.expr(\"cols['1']['chunk1']\").alias(\"chunk1\"),\n",
    "        F.expr(\"cols['1']['entity2']\").alias(\"entity2\"),\n",
    "#        F.expr(\"cols['1']['entity2_begin']\").alias(\"entity2_begin\"),\n",
    "#        F.expr(\"cols['1']['entity2_end']\").alias(\"entity2_end\"),\n",
    "        F.expr(\"cols['1']['chunk2']\").alias(\"chunk2\"),\n",
    "        F.expr(\"cols['1']['confidence']\").alias(\"confidence\")\n",
    "        )\n",
    "\n",
    "result_df.show(n=10, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e60449",
   "metadata": {},
   "source": [
    "### Create a light pipeline for annotating free text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c46eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = re_mets_pipeline_model\n",
    "light_model = LightPipeline(model)\n",
    "\n",
    "annotations = light_model.fullAnnotate(text)\n",
    "\n",
    "rel_df = get_relations_df(annotations,\"mets_relations\")\n",
    "rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da019e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis= RelationExtractionVisualizer()\n",
    "vis.display(annotations[0], 'mets_relations', show_relations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bffaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f9c030b",
   "metadata": {},
   "source": [
    "## Get prediction with sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902472fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_csv(\"./inference/sample.csv\", usecols=['sn_report_number','Report','report_date','Conclusion'])\n",
    "df_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44adb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null text\n",
    "df_text.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null\n",
    "df_text['Conclusion'] = df_text['Conclusion'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade18756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the re visualisation to html file for review\n",
    "# save the re annotation to csv for review\n",
    "for i in range(df_text['sn_report_number'].count()):\n",
    "    text = df_text['Conclusion'].loc[i]\n",
    "    annotations = light_model.fullAnnotate(text)\n",
    "    temp_df = get_relations_df(annotations,'mets_relations')\n",
    "    print(temp_df)\n",
    "    # write to csv\n",
    "    temp_df.to_csv('./inference/display_result/'+df_text['sn_report_number'].iloc[i]+'_tabular.csv')\n",
    "    vis.display(annotations[0], 'mets_relations', show_relations=True, save_path=\"./inference/display_result/\"+df_text['sn_report_number'].loc[i]+\"_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcbf76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b93ce83d",
   "metadata": {},
   "source": [
    "## Get Prediction (testset 460 reports for manuscritpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(\"./inference/testset_460.csv\")\n",
    "input_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdf70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"conclusion\"] = input_df[\"conclusion\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50475d4c",
   "metadata": {},
   "source": [
    "## Post Processing to derive site of mets (term normalization)\n",
    "mapping files:\n",
    "Refer README_mapping_file doc on how to use these files.\n",
    "- 01_map_anatomical_descriptor_to_body_part.csv\n",
    "- 02_map_cancer_body_part_anat.csv\n",
    "- 03_map_normalized_body_part.csv\n",
    "- 04_map_merge_body_part.csv\n",
    "- 05_map_drop_body_part.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999f998",
   "metadata": {},
   "source": [
    "### Import mapping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75da704",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"./mapping/01_map_anatomical_descriptor_to_body_part.csv\")\n",
    "# to convert all to lower case \n",
    "temp_df.anatomical_descriptor = temp_df.anatomical_descriptor.str.lower().str.rstrip()\n",
    "temp_df.map_to = temp_df.map_to.str.lower().str.rstrip()\n",
    "\n",
    "map_anat_descriptor_dict = dict(zip(list(temp_df.anatomical_descriptor), list(temp_df.map_to)))\n",
    "#map_anat_descriptor_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"./mapping/03_map_normalized_body_part.csv\")\n",
    "# to convert all to lower case \n",
    "temp_df.body_part = temp_df.body_part.str.lower().str.rstrip()\n",
    "temp_df.normalized_body_part = temp_df.normalized_body_part.str.lower()\n",
    "map_nbody_part_dict = dict(zip(list(temp_df.body_part), list(temp_df.normalized_body_part)))\n",
    "#map_nbody_part_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439407a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbody_part_df = pd.read_csv(\"./mapping/04_map_merge_body_part.csv\")\n",
    "# to convert all to lower case \n",
    "mbody_part_df.body_part1 = mbody_part_df.body_part1.str.lower().str.rstrip()\n",
    "mbody_part_df.body_part2 = mbody_part_df.body_part2.str.lower().str.rstrip()\n",
    "mbody_part_df.output_body_part = mbody_part_df.output_body_part.str.lower().str.rstrip()\n",
    "mbody_part_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_cancer_chunk = pd.read_csv(\"./mapping/02_map_cancer_body_part_anat.csv\")\n",
    "# to convert all to lower case \n",
    "map_cancer_chunk.cancer_imaging_findings = map_cancer_chunk.cancer_imaging_findings.str.lower().str.rstrip()\n",
    "map_cancer_chunk.body_part_anat = map_cancer_chunk.body_part_anat.str.lower().str.rstrip()\n",
    "map_cancer_chunk = map_cancer_chunk.drop_duplicates()\n",
    "map_cancer_chunk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_to_drop = pd.read_csv(\"./mapping/05_map_drop_body_part.csv\")\n",
    "# to convert all to lower case \n",
    "map_to_drop.body_part_to_drop = map_to_drop.body_part_to_drop.str.lower().str.rstrip()\n",
    "map_to_drop = map_to_drop.drop_duplicates()\n",
    "map_to_drop = set(map_to_drop.body_part_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438469bd",
   "metadata": {},
   "source": [
    "### post processing logic based on dataframe approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping02_bpart(cancer,bpart, df_mapping02):\n",
    "    \n",
    "    # find mapping for the input cancer chunk\n",
    "    df_cancer_map = df_mapping02[df_mapping02[\"cancer_imaging_findings\"]==cancer]\n",
    "    #print(df_cancer_map)\n",
    "    \n",
    "    # check if cancer chunk is found in mapping file.\n",
    "    # if yes, search and get the output_body_part\n",
    "    if len(df_cancer_map) > 0:\n",
    "        \n",
    "        for idx,row in df_cancer_map.iterrows():\n",
    "\n",
    "            # convert string to list\n",
    "            map_list = row[\"body_part_anat\"].split(',')\n",
    "            map_list = [x.strip() for x in map_list]\n",
    "            #print(map_list)\n",
    "\n",
    "            # search bdpart in the list of values. if found, return the output_body_part\n",
    "            # if not found, return \"\"\n",
    "\n",
    "            if bpart in map_list:\n",
    "\n",
    "                # 22-mar-2023 handle no value in output_body_part (eg cake + omental return no value)\n",
    "                if row[\"output_body_part\"] != \"\":\n",
    "                    print(\"***map \",row[\"cancer_imaging_findings\"],\"+\",bpart,\"to \",row[\"output_body_part\"])               \n",
    "                    return(row[\"output_body_part\"])\n",
    "                else:\n",
    "                    print(\"***map \",row[\"cancer_imaging_findings\"],\"+\",bpart,\"to no value\")  \n",
    "                    return(\"\")\n",
    "            else:\n",
    "                # bdpart not found in list, check new row\n",
    "                #print(cancer + \"+\" + bpart + \" not found in mapping 02\")\n",
    "                #return(bpart)\n",
    "                pass\n",
    "        # after processing all available rows, still not found\n",
    "        print(cancer + \"+\" + bpart + \" not found in mapping 02\")\n",
    "        return(bpart)        \n",
    "    else:\n",
    "        print(cancer + \" not found in mapping 02\")\n",
    "        # return original body_part\n",
    "        return(bpart)\n",
    "    \n",
    "df_mapping02 = map_cancer_chunk    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfcaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mets_df = input_df.copy()\n",
    "mets_df[\"pred_site_of_mets\"]=\"\"\n",
    "\n",
    "for i in range(input_df['conclusion'].count()):\n",
    "   \n",
    "    print(\"row: \", i)\n",
    "    \n",
    "    # clean text\n",
    "    text = input_df['conclusion'].loc[i].lower()\n",
    "    print(text)\n",
    "\n",
    "    # get predictions\n",
    "    annotations = light_model.fullAnnotate(text)\n",
    "    rel_df = get_relations_df(annotations,\"mets_relations\")\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 1 - get relation: cancer_imaging_findings > body_part (both direction), append to df\n",
    "    # columns cancer_imaging_findings body_part\n",
    "    ###############################################################################################    \n",
    "    # get cancer_imaging_findings - body_part (both direction)\n",
    "    print(\"\\n-------------------- step 1 --------------------\")\n",
    "    print(\"***get cancer to body_part\\n\")\n",
    "    pair1 = rel_df[(rel_df['entity1']=='cancer_imaging_findings') & (rel_df['entity2']=='body_part')][['chunk1','chunk2']]\n",
    "    pair2 = rel_df[(rel_df['entity2']=='cancer_imaging_findings') & (rel_df['entity1']=='body_part')][['chunk2','chunk1']]\n",
    "    pair2 = pair2.rename(columns={\"chunk2\":\"chunk1\", \"chunk1\":\"chunk2\"})\n",
    "    df_cancer_bpart = pd.concat([pair1,pair2])\n",
    "    df_cancer_bpart = df_cancer_bpart.rename(columns={\"chunk1\":\"cancer_imaging_findings\", \"chunk2\":\"body_part\"})\n",
    "    print(df_cancer_bpart)\n",
    "\n",
    "    # check for lymphangitis carcinomatosis, add pulmonary\n",
    "    s = \"lymphangitis carcinomatosis\"\n",
    "    if s in rel_df[rel_df['entity1']=='cancer_imaging_findings']['chunk1'].str.lower().values.tolist():\n",
    "        print(\"***found \",s, \"add pulmonary\")\n",
    "        to_append = [s,\"pulmonary\"]\n",
    "        df_length = len(df_cancer_bpart)\n",
    "        df_cancer_bpart.loc[df_length] = to_append    \n",
    "    elif s in rel_df[rel_df['entity2']=='cancer_imaging_findings']['chunk2'].str.lower().values.tolist():\n",
    "        print(\"***found \",s, \"add pulmonary\")\n",
    "        to_append = [s,\"pulmonary\"]\n",
    "        df_length = len(df_cancer_bpart)\n",
    "        df_cancer_bpart.loc[df_length] = to_append   \n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 2a - get relation: cancer_imaging_findings > anatomical_descriptor (both direction)\n",
    "    # columns cancer_imaging_findings anatomical_descriptor body_part\n",
    "    ###############################################################################################\n",
    "    print(\"\\n-------------------- step 2a --------------------\")   \n",
    "    print(\"***get cancer to anatomical_descriptor\\n\")\n",
    "    # get cancer_imaging_findings - anatomical_descriptor (need begin/end to link to bodypart) (both direction)\n",
    "    pair1 = rel_df[(rel_df['entity1']=='cancer_imaging_findings') & (rel_df['entity2']=='anatomical_descriptor')][['chunk1','chunk2','entity2_begin','entity2_end']]\n",
    "    pair2 = rel_df[(rel_df['entity2']=='cancer_imaging_findings') & (rel_df['entity1']=='anatomical_descriptor')][['chunk2','chunk1','entity1_begin','entity1_end']]\n",
    "    pair2 = pair2.rename(columns={\"chunk2\":\"chunk1\", \"chunk1\":\"chunk2\",\"entity1_begin\":\"entity2_begin\",\"entity1_end\":\"entity2_end\"})\n",
    "    df_cancer_anat = pd.concat([pair1,pair2])\n",
    "    df_cancer_anat = df_cancer_anat.rename(columns={\"chunk1\":\"cancer_imaging_findings\", \"chunk2\":\"anatomical_descriptor\",\"entity2_begin\":\"anat_begin\",\"entity2_end\":\"anat_end\"})\n",
    "    print(df_cancer_anat)\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 2b - get relation: anatomical descriptor > body_part (both direction)\n",
    "    # columns cancer_imaging_findings anatomical_descriptor anat_begin anat_end\n",
    "    ###############################################################################################\n",
    "    print(\"\\n-------------------- step 2b --------------------\")   \n",
    "    print(\"***get anatomical_descriptor to body_part\\n\") \n",
    "    # get cancer_imaging_findings - anatomical_descriptor (need begin/end to link to bodypart) (both direction)\n",
    "    pair1 = rel_df[(rel_df['entity1']=='anatomical_descriptor') & (rel_df['entity2']=='body_part')][['chunk1','chunk2','entity1_begin','entity1_end']]\n",
    "    pair2 = rel_df[(rel_df['entity2']=='anatomical_descriptor') & (rel_df['entity1']=='body_part')][['chunk2','chunk1','entity2_begin','entity2_end']]\n",
    "    pair2 = pair2.rename(columns={\"chunk2\":\"chunk1\", \"chunk1\":\"chunk2\", \"entity2_begin\":\"entity1_begin\", \"entity2_end\":\"entity1_end\"})\n",
    "    df_anat_bpart = pd.concat([pair1,pair2])\n",
    "    df_anat_bpart = df_anat_bpart.rename(columns={\"chunk1\":\"anatomical_descriptor\", \"entity1_begin\":\"anat_begin\",\"entity1_end\":\"anat_end\",\"chunk2\":\"body_part\"})\n",
    "    print(df_anat_bpart)\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 2c1 - link cancer > anatomical descriptor > body_part (2 hops), get body_part\n",
    "    # link based on anatomatical_descriptor+begin+end\n",
    "    # this is to get cancer > anatomical_descriptor > body_part\n",
    "    # if body_part = NaN, means this anatomical_descriptor has no link to body part, exclude (step 2c2 will process mapping file 01)\n",
    "    ###############################################################################################\n",
    "    print(\"\\n-------------------- step 2c1 --------------------\")      \n",
    "    print(\"***get cancer to anatomical_descriptor to body_part (RE-2 hops)\") \n",
    "    df_cancer_anat_bpart1 = pd.merge(df_cancer_anat,df_anat_bpart,how=\"inner\")\n",
    "    print(df_cancer_anat_bpart1)\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 2c2 - cancer > anatomical descriptor > map body_part (mapping file 01)\n",
    "    ###############################################################################################\n",
    "    print(\"\\n-------------------- step 2c2 --------------------\")      \n",
    "    print(\"***get cancer to anatomical_descriptor to mapped body_part (mapping file 01)\") \n",
    "\n",
    "    # make a copy of df_cancer_anat to store the mapped body_part\n",
    "    df_cancer_anat_bpart2 = df_cancer_anat.copy()\n",
    "    df_cancer_anat_bpart2[\"body_part\"] = \"\"\n",
    "\n",
    "    for idx,row in df_cancer_anat_bpart2.iterrows():\n",
    "        c = row['anatomical_descriptor'].lower()\n",
    "\n",
    "        # check for segment*, add hepatic\n",
    "        #print(\"***checking for segment*: \",c)\n",
    "        if (c.find(\"segment\") != -1) or (c==\"segment\"):\n",
    "            print(\"***found segment*, update body_part to hepatic for segment*\")\n",
    "            df_cancer_anat_bpart2.at[idx,'body_part'] = \"hepatic\"\n",
    "\n",
    "        else:\n",
    "            # look up c in mapping file 01   \n",
    "            try:\n",
    "                v = map_anat_descriptor_dict[c]\n",
    "                print(\"***map \",c,\" -> \",v)\n",
    "                df_cancer_anat_bpart2.at[idx,'body_part'] = v\n",
    "                print(\"row updated\")\n",
    "            except:\n",
    "                pass    \n",
    "\n",
    "    # fill null with \"\", and filter for rows with body_part values \n",
    "    df_cancer_anat_bpart2 = df_cancer_anat_bpart2.fillna(\"\")\n",
    "    df_cancer_anat_bpart2 = df_cancer_anat_bpart2[df_cancer_anat_bpart2[\"body_part\"]!=\"\"]   \n",
    "    print(df_cancer_anat_bpart2)\n",
    "\n",
    "    # resolve conflict between df_cancer_anat_bpart1(via link, prediction may be wrong), df_cancer_anat_bpart2(via map, more accurate)\n",
    "    # logic1-take all from df_cancer_anat_bpart2, union remaining from df_cancer_anat_bpart1\n",
    "    print(\"\\n***resolving conflict between linked body_part / mapped body_part\")\n",
    "    df_cancer_anat_bpart1 = df_cancer_anat_bpart1.rename(columns={\"body_part\":\"linked_body_part\"})\n",
    "    df_cancer_anat_bpart2= df_cancer_anat_bpart2.rename(columns={\"body_part\":\"mapped_body_part\"})\n",
    "    df_step2_cancer_bpart = pd.merge(df_cancer_anat_bpart1,df_cancer_anat_bpart2, how=\"outer\")\n",
    "    df_step2_cancer_bpart[\"body_part\"] = np.where(df_step2_cancer_bpart[\"mapped_body_part\"].notnull(), df_step2_cancer_bpart[\"mapped_body_part\"], df_step2_cancer_bpart[\"linked_body_part\"])\n",
    "    print(df_step2_cancer_bpart)\n",
    "\n",
    "    # merge with step 1 body_part\n",
    "    df_step2_cancer_bpart = pd.concat([df_cancer_bpart,df_step2_cancer_bpart])\n",
    "\n",
    "    # drop duplicate values\n",
    "    df_step2_cancer_bpart = df_step2_cancer_bpart[[\"cancer_imaging_findings\",\"body_part\"]].drop_duplicates()\n",
    "    print(df_step2_cancer_bpart)   \n",
    "       \n",
    "    # remove \\n in body_part\n",
    "    df_step2_cancer_bpart[\"body_part\"] = df_step2_cancer_bpart[\"body_part\"].str.replace(\"\\n\",\"\",regex=False)\n",
    "    \n",
    "    # reset index\n",
    "    df_step2_cancer_bpart = df_step2_cancer_bpart.reset_index(drop=True)\n",
    "    print(\"\\n>>>step 2 sites after resolving anatomical to body_part: \\n\")\n",
    "    print(df_step2_cancer_bpart)\n",
    "\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 3 - process mapping file 02 map cancer+body_part > body_part\n",
    "    # 22-mar-2023: to handle no return value (eg cake + omental > no value)\n",
    "    ###############################################################################################  \n",
    "    print(\"\\n-------------------- step 3 --------------------\")      \n",
    "    print(\"***map cancer+body_part > output_body_part (mapping file 02)\\n\") \n",
    "\n",
    "    # use function to search against mapping 02\n",
    "    df_step2_cancer_bpart['02_body_part']=\"\"\n",
    "    for idx,row in df_step2_cancer_bpart.iterrows():\n",
    "        print(idx, row[\"cancer_imaging_findings\"],row[\"body_part\"])\n",
    "        output_body_part = get_mapping02_bpart(row[\"cancer_imaging_findings\"],row[\"body_part\"], df_mapping02)\n",
    "        #print(output_body_part)\n",
    "        df_step2_cancer_bpart.at[idx,'02_body_part'] = output_body_part\n",
    "\n",
    "    # fill null with \"\", and filter for rows with 02_body_part values \n",
    "    df_step2_cancer_bpart = df_step2_cancer_bpart.fillna(\"\")\n",
    "    df_step2_cancer_bpart = df_step2_cancer_bpart[df_step2_cancer_bpart[\"02_body_part\"]!=\"\"]   \n",
    "\n",
    "    # use set to get all 02_body_part \n",
    "    site = set(df_step2_cancer_bpart['02_body_part'])\n",
    "\n",
    "    print(\"\\n>>>step 3 sites after cancer+body_part mapping: \", site)\n",
    "\n",
    "    ###############################################################################################        \n",
    "    # step 4 - process mapping file 03 map body_part to normalized_body_part\n",
    "    ############################################################################################### \n",
    "    print(\"\\n-------------------- step 4 --------------------\")      \n",
    "    print(\"***map body_part to normalized_body_part (mapping file 03)\\n\")\n",
    "    print(\"current site: \", site)\n",
    "    tempsite=set(site)\n",
    "    newsite=set()\n",
    "    if tempsite == set():\n",
    "        newsite=set()\n",
    "    else:\n",
    "        for s in tempsite:\n",
    "            try:\n",
    "                v = map_nbody_part_dict[s]\n",
    "                print(s, \"-> \",v)\n",
    "                newsite.add(v) \n",
    "            except:\n",
    "                # no map, append current site\n",
    "                print(s)\n",
    "                newsite.add(s)                 \n",
    "    print(\"\\n>>>step 4 sites after normalized body_part: \", newsite)\n",
    "\n",
    "    ###############################################################################################    \n",
    "    # step 5 - process mapping file 04 merge body part\n",
    "    ##############################################################################################      \n",
    "    print(\"\\n-------------------- step 5 --------------------\")      \n",
    "    print(\"***merge body part (mapping file 04)\\n\")\n",
    "    for index, row in mbody_part_df.iterrows():\n",
    "        #print(row)\n",
    "        combine_site=set([row.body_part1,row.body_part2])\n",
    "        if len(newsite.intersection(combine_site)) == 2:\n",
    "            print(\"to merge :\", combine_site)\n",
    "            newsite = newsite.difference(combine_site)\n",
    "            newsite.add(row.output_body_part)\n",
    "            print(\"\\n>>>step 5 sites after merge body_part: \", newsite)\n",
    "\n",
    "    ###############################################################################################                \n",
    "    # step 6- remove nodal if it still exists in output\n",
    "    # process mapping file 05 drop body part values\n",
    "    ###############################################################################################        \n",
    "    #newsite = newsite.difference({'nodal'})\n",
    "    print(\"\\n-------------------- step 6 --------------------\")         \n",
    "    print(\"***drop body part (mapping file 05)\\n\")\n",
    "    sites_to_drop = newsite.intersection(map_to_drop)\n",
    "    if len(sites_to_drop) > 0:\n",
    "        print(\">>>dropping site from mapping 05: \",sites_to_drop )\n",
    "    newsite = newsite.difference(map_to_drop)\n",
    "    print(\"\\n>>>step 6 sites after merge body_part: \", newsite, \"\\n\")\n",
    "    \n",
    "    print(\"\\n>>>final sites to output :\", newsite, \"\\n\")       \n",
    "\n",
    "    mets_df.at[i, 'pred_site_of_mets'] = set(newsite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = input_filename.replace(\".csv\",\"_pred_site_of_mets.csv\")\n",
    "print(\"saving predictions to: \", output_filename)\n",
    "mets_df.to_csv(\"./inference/\"+output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check visualisation for specific case\n",
    "i=0\n",
    "\n",
    "text = input_df['conclusion'].loc[i].lower()\n",
    "annotations = light_model.fullAnnotate(text)\n",
    "\n",
    "rel_df = get_relations_df(annotations,\"mets_relations\")\n",
    "rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fbfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.display(annotations[0], 'mets_relations', show_relations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48225e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "53620885",
   "metadata": {},
   "source": [
    "## Clinical ReDL\n",
    "https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/10.Clinical_Relation_Extraction.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
