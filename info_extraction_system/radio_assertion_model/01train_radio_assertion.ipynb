{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b72228",
   "metadata": {},
   "source": [
    "## Purpose: To train/evaluate a custom assertion model for cancer_imaging_findings entity\n",
    "entity pairs used:\n",
    "- probability high - cancer imaging findings\n",
    "- probability medium - cancer imaging findings\n",
    "- probability low - cancer imaging findings\n",
    "- probability uncertain - cancer imaging findings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80a7e560",
   "metadata": {},
   "source": [
    "#Notebook directory structure\n",
    "    > dataset : contains input files\n",
    "    > saved_models : contains output assertion models\n",
    "    > assertion_output : model predictions\n",
    "    > assertion_result : performance metrics\n",
    "    > training_logs : training logs\n",
    "    > tf_graphs : tensorflow graph file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde75586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to run to create the subfolders, for the first time\n",
    "#!mkdir tf_graphs saved_models assertion_output assertion_result inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef2923",
   "metadata": {},
   "source": [
    "### Note: Before running this notebook, please configure the following paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a449be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using sparknlp clinical embedding word model\n",
    "# specify your folder containing the downloaded clinical embedding word model file, or you can use .pretrained during training instead to load it online\n",
    "embeddings_clinical_local_path = r\"path\\to\\sparknlp_pretrained\\embeddings_clinical_en_2.4.0_2.4_1580237286004\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your sparknlp online license key-need internet connection\n",
    "# we are using v3.4.2\n",
    "sparknlp_licence_key = r\"..\\sparknlp_licence_key\\yourkey.json\"\n",
    "\n",
    "# specify your sparknlp offline license key-airgap env\n",
    "# we are using v3.4.2\n",
    "sparknlp_airgap_licence_key = r\"..\\sparknlp_licence_key\\yourairgapkey.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccebbf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## configure folder path\n",
    "data_folder = \"dataset\"\n",
    "train_folder = data_folder+\"\\\\02csv\"\n",
    "dataset_name = \"train4522\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b105567",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5f363",
   "metadata": {},
   "source": [
    "Note: Requires Spark NLP and Spark NLP for Healthcare (licensed version) packages to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd737e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, sparknlp, sparknlp_jsl, datetime, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.training import CoNLL\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp_jsl.training import tf_graph\n",
    "from sparknlp_display import AssertionVisualizer, NerVisualizer \n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6ea39",
   "metadata": {},
   "source": [
    "Note: Requires Spark NLP for Healthcare (licensed version) license key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdacb53",
   "metadata": {},
   "source": [
    "### Start Spark Session (Offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline-Load airgap license key\n",
    "with open(sparknlp_airgap_licence_key) as f:\n",
    "    airgap_license_keys = json.load(f)\n",
    "    \n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(airgap_license_keys)\n",
    "os.environ.update(airgap_license_keys)\n",
    "\n",
    "# check variable\n",
    "!echo $SECRET\n",
    "!echo $JSL_VERSION\n",
    "!echo $PUBLIC_VERSION\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])\n",
    "\n",
    "# Start Spark Session with Custom Params (OFFLINE)\n",
    "def start(SECRET):\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed radio_assertion\") \\\n",
    "        .master(\"local[16]\") \\\n",
    "        .config(\"spark.driver.memory\", \"16G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.driver.maxResultSize\",\"4000M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\") \\\n",
    "        .config(\"spark.jars\", f\"d:\\content\\spark-nlp-jsl-{JSL_VERSION}.jar, d:\\content\\spark-nlp_2.12-3.4.2.jar\" )\n",
    "\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = start(SECRET) \n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b33867",
   "metadata": {},
   "source": [
    "### Start Spark Session (Online)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bca30290",
   "metadata": {},
   "source": [
    "# Load license key\n",
    "with open(sparknlp_licence_key) as f:\n",
    "    license_keys = json.load(f)\n",
    "license_keys.keys()\n",
    "\n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "\n",
    "# Adding license key-value pairs to environment variables, use this (17-Aug-2022)\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0c857eb",
   "metadata": {},
   "source": [
    "## 17-Aug-2022: try this, this is working\n",
    "# Start Spark Session\n",
    "params = {\"spark.master\":\"local[16]\",\n",
    "          \"spark.driver.memory\":\"16G\", \n",
    "          \"spark.executor.memory\":\"2G\",\n",
    "          \"spark.kryoserializer.buffer.max\":\"2000M\", \n",
    "          \"spark.driver.maxResultSize\":\"4000M\",\n",
    "          \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "          \"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\",\n",
    "          \"spark.jars\": \"https://pypi.johnsnowlabs.com/\"+SECRET+\"/spark-nlp-jsl-\"+JSL_VERSION+\".jar\",\n",
    "          \"spark.local.dir\": r\"c:\\users\\guathwa\\spark-temp\"\n",
    "           } \n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81d4afaf",
   "metadata": {},
   "source": [
    "# check variable\n",
    "#!echo $SECRET\n",
    "!echo $JSL_VERSION\n",
    "!echo $PUBLIC_VERSION\n",
    "#!echo $SPARK_NLP_LICENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac72659",
   "metadata": {},
   "source": [
    "## Import train csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2cb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import train csv\n",
    "processed_df = pd.read_csv(os.path.join(train_folder,\"assertion_traintest.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e41101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "#Create User defined Custom Schema using StructType\n",
    "dfSchema = StructType([StructField(\"text\", StringType(), True)\\\n",
    "                        ,StructField(\"target\", StringType(), True)\\\n",
    "                        ,StructField(\"label\", StringType(), True)\\\n",
    "                        ,StructField(\"start\", IntegerType(), True)\\\n",
    "                        ,StructField(\"end\", IntegerType(), True)\\\n",
    "                        ,StructField(\"doc_title\", StringType(), True)\\\n",
    "                        ,StructField(\"dataset\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_df=spark.createDataFrame(processed_df, schema=dfSchema) \n",
    "assertion_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd300ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_df.show(3, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0054bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = assertion_df.where(\"dataset='train'\")\n",
    "test_data = assertion_df.where(\"dataset='test'\")\n",
    "\n",
    "#===================================================\n",
    "print(\"total row count:\",assertion_df.count())\n",
    "print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))\n",
    "\n",
    "trainset_count = training_data.groupby('label').count().collect()\n",
    "testset_count = test_data.groupby('label').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72517877",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.groupBy('label').count().orderBy('count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.groupBy('label').count().orderBy('count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7eac56",
   "metadata": {},
   "source": [
    "## ------------------- START OF TRAINING --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4f6dd",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "chunk = Doc2Chunk()\\\n",
    "    .setInputCols(\"document\")\\\n",
    "    .setOutputCol(\"chunk\")\\\n",
    "    .setChunkCol(\"target\")\\\n",
    "    .setStartCol(\"start\")\\\n",
    "    .setStartColByTokenIndex(False)\\\n",
    "    .setFailOnMissing(False)\\\n",
    "    .setLowerCase(True)\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "clinical_embeddings = WordEmbeddingsModel.load(embeddings_clinical_local_path)\\\n",
    "        .setInputCols([\"document\", \"token\"])\\\n",
    "        .setOutputCol(\"embeddings\")\n",
    "\n",
    "clinical_assertion_pipeline = Pipeline(\n",
    "    stages = [\n",
    "    document,\n",
    "    chunk,\n",
    "    token,\n",
    "    clinical_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_test_data = clinical_assertion_pipeline.fit(test_data).transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfa33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assertion_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f75b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testfile = 'assertion_test_data.parquet'\n",
    "assertion_test_data.write.parquet(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b55d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Output Dataframe\n",
    "output_df = pd.DataFrame(columns = ['assertion_model','ner_model','trainset_count','testset_count','epoch', 'learning_rate', 'batch_size','start_time', 'end_time', 'duration', 'overall_accuracy','class_accuracy', 'classification_report','confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed4467",
   "metadata": {},
   "source": [
    "## Start Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "#epoch = 5\n",
    "epoch = 20\n",
    "#epoch = 15\n",
    "#batch_size = 8\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "#learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================\n",
    "# start training\n",
    "#======================================\n",
    "start = time.ctime()\n",
    "start2 = time.time()\n",
    "print('start time for training: ', start)\n",
    "print('...setup training pipeline')\n",
    "\n",
    "graph_folder= \"./tf_graphs\"\n",
    "scope_window = [15,10]\n",
    "#.setGraphFile(f\"{graph_folder}/blstm_34_32_30_200_4.pb\")\\\n",
    "\n",
    "# 03-apr-2023 add     .setIncludeConfidence(True)\\\n",
    "# 03-apr-2023 add     .setRandomSeed (not available in v3.4.2)\n",
    "assertionStatus = AssertionDLApproach()\\\n",
    "    .setLabelCol(\"label\")\\\n",
    "    .setInputCols(\"document\", \"chunk\", \"embeddings\")\\\n",
    "    .setOutputCol(\"assertion\")\\\n",
    "    .setBatchSize(batch_size)\\\n",
    "    .setDropout(0.1)\\\n",
    "    .setLearningRate(learning_rate)\\\n",
    "    .setEpochs(epoch)\\\n",
    "    .setValidationSplit(0.1)\\\n",
    "    .setStartCol(\"start\")\\\n",
    "    .setEndCol(\"end\")\\\n",
    "    .setMaxSentLen(250)\\\n",
    "    .setIncludeConfidence(True)\\\n",
    "    .setEnableOutputLogs(True)\\\n",
    "    .setOutputLogsPath('training_logs/')\\\n",
    "    .setGraphFolder(graph_folder)\\\n",
    "    .setTestDataset(path=testfile, read_as='SPARK', options={'format': 'parquet'})\\\n",
    "    .setScopeWindow(scope_window)\n",
    "\n",
    "clinical_assertion_pipeline = Pipeline(\n",
    "    stages = [\n",
    "    document,\n",
    "    chunk,\n",
    "    token,\n",
    "    clinical_embeddings,\n",
    "    assertionStatus])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "assertion_model = clinical_assertion_pipeline.fit(training_data)\n",
    "\n",
    "print('...training completed')\n",
    "done = time.ctime()\n",
    "done2 = time.time()\n",
    "duration = done2-start2\n",
    "print('end time for training: ', done)\n",
    "#======================================\n",
    "# end training\n",
    "# Wall time: 5min 47s (10 epochs)\n",
    "#======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2eced8",
   "metadata": {},
   "source": [
    "## save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "assertion_model_name = \"assertion_model_\"+str(epoch)+\"_\"+str(batch_size)+\"_\"+str(learning_rate)+\"_\"+str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")+\"_\"+dataset_name)\n",
    "print('...save models to folder: ./saved_models/'+assertion_model_name)\n",
    "assertion_model.stages[-1].write().overwrite().save('./saved_models/'+assertion_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3408c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below codes on model evaluation has error, need to upgrade to sparknlp 4.x (this notebook is using v3.4.2)\n",
    "# current workaround: use lightpipeline to annotate and get the prediction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bef4454",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "log_files = os.listdir(\"training_logs\")\n",
    "log_files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecfda70e",
   "metadata": {},
   "source": [
    "with open(\"training_logs/\"+log_files[0]) as log_file:\n",
    "    print(log_file.read())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf19691f",
   "metadata": {},
   "source": [
    "# get test performance (this is successful but subsequent steps failed with error, fixed in v4.x)\n",
    "preds = assertion_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a1c2304",
   "metadata": {},
   "source": [
    "preds.select('text', preds.chunk.result.alias('chunk'), 'label',preds.assertion.result.alias('prediction')).show(30)\n",
    "\n",
    "#error\n",
    "#Caused by: java.lang.IllegalArgumentException: requirement failed: Could not convert chunk index to token index because could not find chunk with offsets 124 - 137 within sentence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "116573b4",
   "metadata": {},
   "source": [
    "#preds.select('label',preds.assertion.result.alias('prediction')).collect()\n",
    "test = preds.select('text','label',preds.assertion).collect()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2be45652",
   "metadata": {},
   "source": [
    "preds_df = preds.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d277b40",
   "metadata": {},
   "source": [
    "## Model Evaluation using lightpipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the name of NER model\n",
    "radio_ner_model = \"clinical_embeddings_5_8_0.001_u0.4o1_train4522\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1fcb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "document = DocumentAssembler()\\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "        .setInputCols(['document'])\\\n",
    "        .setOutputCol('sentences')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "        .setInputCols(['sentences'])\\\n",
    "        .setOutputCol('tokens')\n",
    "\n",
    "words_embedder = WordEmbeddingsModel()\\\n",
    "    .load(embeddings_clinical_local_path)\\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "  \n",
    "radio_ner_tagger = MedicalNerModel.load(radio_ner_model)\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"ner_tags\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "        .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "        .setOutputCol(\"ner_span\")\\\n",
    "        .setWhiteList([\"cancer_imaging_findings\"])\n",
    "\n",
    "## add radio assertion model\n",
    "radiology_assertion = AssertionDLModel.load('./saved_models/'+assertion_model_name) \\\n",
    "    .setInputCols([\"sentences\", \"ner_span\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion\")\n",
    "\n",
    "ner_assertion_pipeline = Pipeline(stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        token,\n",
    "        words_embedder,\n",
    "        radio_ner_tagger,\n",
    "        converter,\n",
    "        radiology_assertion\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "\n",
    "ner_assertion_model = ner_assertion_pipeline.fit(empty_data)\n",
    "\n",
    "lmodel = LightPipeline(ner_assertion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eac34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================\n",
    "# model evaluation\n",
    "#======================================\n",
    "print('...evaluate model')\n",
    "\n",
    "# use the light model to get testset prediction\n",
    "test_df = test_data.toPandas()\n",
    "test_df = test_df.reset_index()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a827bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "visualiser = NerVisualizer()\n",
    "\n",
    "i=21\n",
    "#i=942\n",
    "#i=948\n",
    "ppres = lmodel.fullAnnotate(test_df['text'].loc[i])[0]\n",
    "visualiser.display(ppres, label_col='ner_span', document_col='document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_vis = AssertionVisualizer()\n",
    "assertion_vis.display(ppres, 'ner_span', 'assertion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295db4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppres['assertion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519bebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppres['ner_span']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppres['assertion'][0].metadata['confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbaf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03-apr-2023 add confidence to output, for manuscript\n",
    "chunk=[]\n",
    "entity=[]\n",
    "status=[]\n",
    "confidence=[]\n",
    "for n,m in zip(ppres['ner_span'],ppres['assertion']):\n",
    "    chunk.append(n.result)\n",
    "    entity.append(n.metadata['entity']) \n",
    "    status.append(m.result)\n",
    "    confidence.append(m.metadata['confidence'])\n",
    "\n",
    "temp_df = pd.DataFrame({'index':test_df['index'].loc[i],'text':test_df['text'].loc[i],'target':test_df['target'].loc[i],'label':test_df['label'].loc[i],'chunk':chunk, 'entity':entity, 'assertion_prediction':status, 'confidence':confidence})    \n",
    "temp_df['entity_index'] = temp_df.index\n",
    "temp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03-apr-2023 add confidence to output, for manuscript\n",
    "# get performance on test set\n",
    "#####################################\n",
    "preds_df = pd.DataFrame()\n",
    "\n",
    "#for i in [3,4]:\n",
    "for i in range(test_df['text'].count()):\n",
    "    print(i)\n",
    "    ppres = lmodel.fullAnnotate(test_df['text'].loc[i])[0]\n",
    "    #visualiser.display(ppres, label_col='ner_span', document_col='document', save_path=\"./display_result_18oct2022/\"+df_text['sn_report_number'].loc[i]+\"_report.html\")\n",
    "\n",
    "    #output to csv\n",
    "    chunk=[]\n",
    "    entity=[]\n",
    "    status=[]\n",
    "    confidence=[]\n",
    "    \n",
    "    for n,m in zip(ppres['ner_span'],ppres['assertion']):\n",
    "        chunk.append(n.result)\n",
    "        entity.append(n.metadata['entity']) \n",
    "        status.append(m.result)\n",
    "        confidence.append(m.metadata['confidence'])\n",
    "        \n",
    "    temp_df = pd.DataFrame({'index':test_df['index'].loc[i],'text':test_df['text'].loc[i],'target':test_df['target'].loc[i],'label':test_df['label'].loc[i],'chunk':chunk, 'entity':entity, 'assertion_prediction':status, 'confidence':confidence})         \n",
    "    temp_df['entity_index'] = temp_df.index\n",
    "    print(temp_df)\n",
    "    preds_df = preds_df.append(temp_df)\n",
    "\n",
    "#save prediction to csv, output where target = chunk\n",
    "preds_df = preds_df[preds_df['target'] == preds_df['chunk']]\n",
    "columns = ['index','text', 'target','label', 'entity','assertion_prediction','confidence']\n",
    "\n",
    "filename = assertion_model_name+\"_predictions_wconfidence.csv\"\n",
    "preds_df.to_csv(\"./assertion_output/\"+\"/\"+filename, columns=columns, index=False)   \n",
    "\n",
    "# get performance metrics\n",
    "y_true = preds_df['label']\n",
    "y_pred = preds_df['assertion_prediction']\n",
    "accuracy = accuracy_score(y_true,y_pred)\n",
    "print(\"accuracy: \", accuracy)\n",
    "micro_f1 = f1_score(y_true,y_pred, average=\"micro\")\n",
    "print(\"micro_f1: \", micro_f1)\n",
    "\n",
    "report = classification_report(y_true,y_pred, digits=4, labels=np.unique(y_true))\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_true,y_pred)\n",
    "print(cm)\n",
    "\n",
    "# get per class accuracy\n",
    "# https://stackoverflow.com/questions/39770376/scikit-learn-get-accuracy-scores-for-each-class\n",
    "classes=np.unique(y_true)\n",
    "\n",
    "# We will store the results in a dictionary for easy access later\n",
    "per_class_accuracies = {}\n",
    "\n",
    "# Calculate the accuracy for each one of our classes\n",
    "for idx, cls in enumerate(classes):\n",
    "    # True negatives are all the samples that are not our current GT class (not the current row) \n",
    "    # and were not predicted as the current class (not the current column)\n",
    "    true_negatives = np.sum(np.delete(np.delete(cm, idx, axis=0), idx, axis=1))\n",
    "    \n",
    "    # True positives are all the samples of our current GT class that were predicted as such\n",
    "    true_positives = cm[idx, idx]\n",
    "    \n",
    "    # The accuracy for the current class is ratio between correct predictions to all predictions   \n",
    "    # 03-jul-2023: dont consider TN, use TP/(TP+FP+FN), same formulae for whole manuscript\n",
    "    per_class_accuracies[cls] = (true_positives) / (np.sum(cm)-true_negatives) \n",
    "    \n",
    "# Combine class accuracies to classification report\n",
    "report_dict = classification_report(y_true,y_pred, digits=4, labels=np.unique(y_true), output_dict=True)\n",
    "classification_report_df = pd.DataFrame(report_dict).transpose()\n",
    "per_class_accuracies_df = pd.DataFrame.from_dict(per_class_accuracies, orient='index', columns=['class_accuracy']) \n",
    "combine_report_df = pd.concat([per_class_accuracies_df,classification_report_df], axis=1)\n",
    "\n",
    "# save performance to csv\n",
    "# model,rels_set,trainset_count,testset_count,epoch,learning_rate,batch_size,start_time,end_time,duration,accuracy,classification_report,confusion_matrix\n",
    "to_append = [assertion_model_name,radio_ner_model,trainset_count,testset_count,epoch,learning_rate,batch_size,start,done,duration,accuracy,per_class_accuracies_df,report,cm]\n",
    "df_length = len(output_df)\n",
    "\n",
    "output_df.loc[df_length] = to_append\n",
    "filename_prefix = \"./assertion_result/\"+\"/\"+assertion_model_name\n",
    "filename = \"%s.csv\" % filename_prefix\n",
    "output_df.to_csv(filename, header=True)\n",
    "print(50*'-')\n",
    "print(\"<<<Model Performance saved!>>>\")\n",
    "print(50*'-')\n",
    "print(50*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bceedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c892e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all labels\n",
    "print(classification_report(y_true,y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 labels\n",
    "print(classification_report(y_true,y_pred, digits=4, labels=[\"probability_high\",\"probability_medium\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true,y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190b4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62c3b37f",
   "metadata": {},
   "source": [
    "## ------------------- END OF TRAINING--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09d1ea",
   "metadata": {},
   "source": [
    "## ------------------- MODEL INFERENCE --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288cc6c",
   "metadata": {},
   "source": [
    "# 4. Test Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "document = DocumentAssembler()\\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "        .setInputCols(['document'])\\\n",
    "        .setOutputCol('sentences')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "        .setInputCols(['sentences'])\\\n",
    "        .setOutputCol('tokens')\n",
    "\n",
    "words_embedder = WordEmbeddingsModel()\\\n",
    "    .load(embeddings_clinical_local_path)\\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "  \n",
    "radio_ner_tagger = MedicalNerModel.load(radio_ner_model)\\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"ner_tags\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "        .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"])\\\n",
    "        .setOutputCol(\"ner_span\")\\\n",
    "        .setWhiteList([\"cancer_imaging_findings\"])\n",
    "\n",
    "## add radio assertion model\n",
    "radiology_assertion = AssertionDLModel.load('./saved_models/'+'/'+assertion_model_name) \\\n",
    "    .setInputCols([\"sentences\", \"ner_span\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion\")\n",
    "\n",
    "ner_assertion_pipeline = Pipeline(stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        token,\n",
    "        words_embedder,\n",
    "        radio_ner_tagger,\n",
    "        converter,\n",
    "        radiology_assertion\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "\n",
    "ner_assertion_model = ner_assertion_pipeline.fit(empty_data)\n",
    "\n",
    "lmodel = LightPipeline(ner_assertion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6eb406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# site of mets\n",
    "mtext1 = \"\"\"\n",
    "your sample text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4544f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = mtext1\n",
    "sample_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "sample_data.show(truncate=False)\n",
    "sample_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ner_assertion_model.transform(sample_data)\n",
    "\n",
    "preds.select(F.explode(F.arrays_zip(\"ner_span.result\",\"ner_span.metadata\")).alias(\"entities\")) \\\n",
    ".select(F.expr(\"entities['0']\").alias(\"chunk\"),\n",
    "        F.expr(\"entities['1'].entity\").alias(\"entity\")).show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce39c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.select(F.explode(F.arrays_zip(preds.ner_span.result, \n",
    "                                     preds.ner_span.metadata, \n",
    "                                     preds.assertion.result)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunks\"),\n",
    "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
    "              F.expr(\"cols['1']['sentences']\").alias(\"sent_id\"),\n",
    "              F.expr(\"cols['2']\").alias(\"assertion\")).show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085dea44",
   "metadata": {},
   "source": [
    "## LightPipeline / Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! mkdir display_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab375c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppres = lmodel.fullAnnotate(text)[0]\n",
    "ppres.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9debc79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "visualiser = NerVisualizer()\n",
    "visualiser.display(ppres, label_col='ner_span', document_col='document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b558ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_vis = AssertionVisualizer()\n",
    "assertion_vis.display(ppres, 'ner_span', 'assertion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe21a7",
   "metadata": {},
   "source": [
    "## Get prediction with sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575836db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the column names accordinlgy to suit your dataset\n",
    "df_text = pd.read_csv(\"./inference/samples.csv\", usecols=['sn_report_number', 'report_date','findings','conclusion'])\n",
    "df_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb236ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe640d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null text\n",
    "df_text.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80916b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null\n",
    "df_text['conclusion'] = df_text['conclusion'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the visualisation to html file for review\n",
    "# save the annotation to csv for review\n",
    "annotation_df = pd.DataFrame()\n",
    "for i in range(df_text['sn_report_number'].count()):\n",
    "    print(i)\n",
    "    ppres = lmodel.fullAnnotate(df_text['conclusion'].loc[i])[0]\n",
    "    assertion_vis.display(ppres, 'ner_span', 'assertion',save_path=\"./inference/display_result/\"+df_text['sn_report_number'].loc[i]+\"_report.html\")\n",
    "    #output to csv\n",
    "    chunk=[]\n",
    "    entity=[]\n",
    "    status=[]\n",
    "    for n,m in zip(ppres['ner_span'],ppres['assertion']):\n",
    "        chunk.append(n.result)\n",
    "        entity.append(n.metadata['entity']) \n",
    "        status.append(m.result)\n",
    "    temp_df = pd.DataFrame({'sn_report_number':df_text['sn_report_number'].loc[i],'report_date':df_text['report_date'].loc[i],'chunk':chunk, 'entity':entity, 'assertion_status':status})    \n",
    "    temp_df['entity_index'] = temp_df.index\n",
    "    #print(temp_df)\n",
    "    annotation_df = annotation_df.append(temp_df)\n",
    "    #print(annotation_df)\n",
    "\n",
    "columns = ['sn_report_number', 'report_date','entity_index', 'entity','chunk','assertion_status']\n",
    "annotation_df.to_csv(\"./inference/display_result/sample_ner_assertion.csv\", columns=columns, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa64a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e710b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
