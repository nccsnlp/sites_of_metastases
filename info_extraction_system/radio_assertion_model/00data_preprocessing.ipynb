{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b72228",
   "metadata": {},
   "source": [
    "### Purpose: To preprocess/clean train csv for assertion model training\n",
    "entity pairs used:\n",
    "\n",
    "- probability high - cancer imaging findings\n",
    "- probability medium - cancer imaging findings\n",
    "- probability low - cancer imaging findings\n",
    "- probability uncertain - cancer imaging findings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80a7e560",
   "metadata": {},
   "source": [
    "#Notebook directory structure\n",
    "    > dataset : contains input files\n",
    "    > models\n",
    "    > saved_models : contains output assertion models\n",
    "    > assertion_output\n",
    "    > assertion_result\n",
    "    > assertion_logs_ncc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef2923",
   "metadata": {},
   "source": [
    "### Note: Before running this notebook, please configure the following paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccebbf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test setting\n",
    "data_folder = \"dataset\"\n",
    "train_folder = data_folder+\"\\\\02csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your sparknlp online license key-need internet connection\n",
    "# we are using v3.4.2\n",
    "sparknlp_licence_key = r\"..\\sparknlp_licence_key\\yourkey.json\"\n",
    "\n",
    "# specify your sparknlp offline license key-airgap env\n",
    "# we are using v3.4.2\n",
    "sparknlp_airgap_licence_key = r\"..\\sparknlp_licence_key\\yourairgapkey.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b105567",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd737e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, sparknlp, sparknlp_jsl, datetime, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.training import CoNLL\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp_jsl.training import tf_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ab05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session (Offline)\n",
    "# Offline-Load airgap license key\n",
    "with open(sparknlp_airgap_licence_key) as f:\n",
    "    airgap_license_keys = json.load(f)\n",
    "    \n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(airgap_license_keys)\n",
    "os.environ.update(airgap_license_keys)\n",
    "\n",
    "# check variable\n",
    "!echo $SECRET\n",
    "!echo $JSL_VERSION\n",
    "!echo $PUBLIC_VERSION\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])\n",
    "\n",
    "# Start Spark Session with Custom Params (OFFLINE)\n",
    "def start(SECRET):\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed radio_assertion\") \\\n",
    "        .master(\"local[16]\") \\\n",
    "        .config(\"spark.driver.memory\", \"16G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.driver.maxResultSize\",\"4000M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\") \\\n",
    "        .config(\"spark.jars\", f\"d:\\content\\spark-nlp-jsl-{JSL_VERSION}.jar, d:\\content\\spark-nlp_2.12-3.4.2.jar\" )\n",
    "\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "spark = start(SECRET) \n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8139dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ac72659",
   "metadata": {},
   "source": [
    "## Import data\n",
    "- use the same csv file generated by radio_re_model > 02data_preprocessing_v1.0.ipynb\n",
    "- the file can be found in radio_re_model\\dataset\\02csv\\radio_re_train4522_allrelations_clean.csv\n",
    "- copy this file to radio_assertion_model\\dataset\\02csv\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad2a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(os.path.join(train_folder,\"radio_re_train4522_allrelations_clean.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecef120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e24c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for probability_xx - cancer imaging findings pair\n",
    "df_csv = df_csv[df_csv['relation_type']=='sentence']\n",
    "\n",
    "columns = ['sentence','chunk1','entity1','chunk2','entity2','entity2_begin','entity2_end','doc_title','dataset']\n",
    "condition1 = df_csv['entity1'].isin(['probability_high','probability_medium','probability_uncertain','probability_low'])\n",
    "condition2 = df_csv['entity2']==\"cancer_imaging_findings\"\n",
    "df_csv = df_csv[condition1 & condition2][columns]\n",
    "df_csv = df_csv.reset_index(drop=True)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b566e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.groupby([\"entity1\",\"entity2\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a8f9c",
   "metadata": {},
   "source": [
    "## Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8653aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate row\n",
    "df = df_csv.copy()\n",
    "df = df.drop_duplicates().reset_index()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0df587",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ecfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('dataset').count()['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renamed column\n",
    "df.rename({'sentence': 'text', 'chunk2': 'target', 'entity1':'label','entity2_begin':'start','entity2_end':'end'}, axis=1, inplace=True)\n",
    "columns = ['text','target', 'label','start','end','doc_title','dataset']\n",
    "df = df[columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1de86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning, remove punctuation\n",
    "df['text'] = df['text'].str.replace(r'[^\\w\\s]+','',regex=True) \n",
    "df['target'] = df['target'].str.replace(r'[^\\w\\s]+','',regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text using sparknlp tokenizer\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "text_token = Tokenizer()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "tokenizer_pipeline = Pipeline(\n",
    "    stages = [\n",
    "    document,\n",
    "    text_token])\n",
    "\n",
    "assertion_df = spark.createDataFrame(df[['text']]).toDF(\"text\")\n",
    "assertion_token = tokenizer_pipeline.fit(assertion_df).transform(assertion_df)\n",
    "assertion_token_df = assertion_token.select('text','token.result').toPandas()\n",
    "assertion_token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the token to csv for checking\n",
    "assertion_token_df.to_csv(os.path.join(train_folder,\"radio_assertion_traintest_token.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82264e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get the token idx for assertion training, i.e the token start/end position in sentence\n",
    "def get_token_idx (idx, target):\n",
    "    target_list = target.split()\n",
    "    print(target_list)\n",
    "    token_list = assertion_token_df.iloc[idx]['result']\n",
    "    print(token_list)\n",
    "    start = token_list.index(target_list[0])\n",
    "    end = start + len(target_list)-1\n",
    "    print(start,end)\n",
    "    return(start,end)\n",
    "    \n",
    "#get_token_idx(637,'hepatocellular carcinoma (hcc)')\n",
    "#get_token_idx(0,'mass')\n",
    "processed_df = df.copy()\n",
    "\n",
    "for i in range(processed_df['text'].count()):\n",
    "    print(\"***\",i)\n",
    "    #word = df.iloc[i]['target'].replace('.','')\n",
    "    word = df.iloc[i]['target']\n",
    "    try:\n",
    "        s,e = get_token_idx(i,word)\n",
    "        #print(s,e)\n",
    "        #print(start,end)\n",
    "        processed_df.at[i,'start'] = s\n",
    "        processed_df.at[i,'end'] = e\n",
    "    except:\n",
    "        print(\"bad row, token not found, set start to -1, this row will be excluded from training\")\n",
    "        processed_df.at[i,'start'] = -1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad rows\n",
    "processed_df[processed_df['start'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873349ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop bad rows\n",
    "processed_df = processed_df[processed_df['start'] != -1]\n",
    "processed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.groupby('dataset').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.groupby(['dataset','label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d4e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv(os.path.join(train_folder,\"radio_assertion_traintest.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932a082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
